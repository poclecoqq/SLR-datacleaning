{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pathlib\n",
    "from collections import Counter, OrderedDict\n",
    "from difflib import SequenceMatcher as SM\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from pywaffle import Waffle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories\n",
    "in_dir = pathlib.Path('./in/')\n",
    "out_dir = pathlib.Path('./out/')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set seaborn style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # Fetch papers\n",
    "    df1 = pd.read_csv(in_dir/'initial_papers.csv')\n",
    "    df1['source'] = 'initial'\n",
    "    df2 = pd.read_csv(in_dir/'snowball_papers.csv')\n",
    "    df2['source'] = 'snowball'\n",
    "    df = pd.concat([df1, df2])\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Filter out irrelevant papers\n",
    "    df = df[(df['Relevant'] != 'NO') & (df['Relevant'] != 'score too low') & (df['Relevant'] != 'not sure') & (df['Relevant'] != 'exclusion criteria') & (df['Relevant'] != 'Not accessible') & (df['Relevant'] != 'not included')]\n",
    "    # Change columns' type\n",
    "    df['Publication Year'] = df['Publication Year'].astype('int32')\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_potential_duplicates_venue_name():\n",
    "    venue_names = df['venue name'].unique()\n",
    "    potential_duplicates = []\n",
    "    for i in range(len(venue_names)):\n",
    "        for j in range(i+1, len(venue_names)):\n",
    "            ratio = SM(None, venue_names[i], venue_names[j]).ratio()\n",
    "            if ratio > .75:\n",
    "                potential_duplicates.append((venue_names[i], venue_names[j]))\n",
    "    return potential_duplicates\n",
    "\n",
    "# potential_duplicates = detect_potential_duplicates_venue_name()\n",
    "# potential_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_venue_most_than_one():\n",
    "    \"\"\"\n",
    "    Returns the venues that more than one paper cited.\n",
    "    \"\"\"\n",
    "    cnt = Counter(df['venue name']).most_common()\n",
    "    d = OrderedDict()\n",
    "    for k,v in cnt:\n",
    "        if v > 1:\n",
    "            d[k] = v\n",
    "    return d\n",
    "\n",
    "d = get_venue_most_than_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_n_papers_venues(d, file_name='venues'):\n",
    "    d_copy = deepcopy(d)\n",
    "    del d_copy['arxiv']\n",
    "    del d_copy['proquest']\n",
    "    ax = sns.barplot(x=list(d_copy.keys()), y=list(d_copy.values()), color=sns.color_palette('colorblind')[0])\n",
    "    ax.set_ylabel('# publications')\n",
    "\n",
    "    # ax.set_axisbelow(True)\n",
    "    # ax.grid(axis='y')\n",
    "\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    if file_name:\n",
    "        plt.savefig(out_dir/f'{file_name}.pdf', bbox_inches='tight')\n",
    "plot_n_papers_venues(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of papers per year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_papers_per_year(df, file_name='years'):\n",
    "    ax = sns.countplot(data=df, x=\"Publication Year\", color=sns.color_palette('colorblind')[0])\n",
    "    ax.set_ylabel('# papers')\n",
    "    ax.set_xlabel('year')\n",
    "    \n",
    "    if file_name:\n",
    "        plt.savefig(out_dir/f'{file_name}.pdf', bbox_inches='tight')\n",
    "\n",
    "plot_papers_per_year(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors' affiliation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_authors_affiliation(df, file_name=\"affiliation\"):\n",
    "    cnt = Counter(df[\"Author's affiliation\"])\n",
    "    keys, values = list(cnt.keys()), list(cnt.values())\n",
    "    # Count percentages\n",
    "    tot = sum(values)\n",
    "    keys = [f'{key}\\n{(value/tot)*100:.0f}%' for key, value in zip(keys, values)]\n",
    "\n",
    "    # plot\n",
    "    _, labels = plt.pie(values, labels=keys, colors = sns.color_palette('colorblind'), startangle=90, labeldistance=1.35)\n",
    "\n",
    "    # Change labels horizontal and vertical alignement\n",
    "    for i, label in enumerate(labels):\n",
    "        label.set_horizontalalignment('center')\n",
    "        if i == 0:\n",
    "            label.set_verticalalignment('bottom')\n",
    "        else:\n",
    "            label.set_verticalalignment('top')\n",
    "\n",
    "\n",
    "\n",
    "    # add a circle at the center to transform it in a donut chart\n",
    "    my_circle=plt.Circle( (0,0), 0.65, color='white')\n",
    "    p=plt.gcf()\n",
    "    p.gca().add_artist(my_circle)\n",
    "\n",
    "    if file_name:\n",
    "        plt.savefig(out_dir/f'{file_name}.pdf', bbox_inches='tight')\n",
    "\n",
    "plot_authors_affiliation(df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per cleaning activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_waffle_plot(df, col_name, file_name=None):\n",
    "    cnt = OrderedDict(Counter(df[col_name].dropna().tolist()).most_common()) \n",
    "    tot = sum(cnt.values())\n",
    "    val = [ 100*(x/tot) for x in cnt.values()]\n",
    "    labels = [ f'{x} ({\"{:.0f}\".format(val[i])}%)' for i,x in enumerate(cnt.keys())]\n",
    "    \n",
    "    fig = plt.figure(\n",
    "        FigureClass=Waffle, \n",
    "        columns=15, \n",
    "        values=cnt, \n",
    "        labels=labels,\n",
    "        colors=sns.color_palette('colorblind')[:len(cnt)],\n",
    "        legend={'loc': 'lower center', 'bbox_to_anchor': (0.5, -0.5), 'fontsize':11, 'ncol': 2,},\n",
    "        icons='book', font_size=25, \n",
    "        icon_legend=True,\n",
    "        block_arranging_style='snake'\n",
    "        # figsize=(6, 6),\n",
    "    )\n",
    "\n",
    "    if file_name:\n",
    "        plt.savefig(out_dir/f'{file_name}.pdf', bbox_inches='tight')\n",
    "\n",
    "def plot_cleaning_activity(df, file_name='task'):\n",
    "\n",
    "    # Preprocess data\n",
    "    df_c = df.copy()\n",
    "    df_c.loc[ df_c['task'] == \"error detection/repair\", 'task'] = 'Feature Cleaning'\n",
    "    df_c.loc[ df_c['task'] == \"error detection\", 'task'] = 'Feature Cleaning'\n",
    "    df_c.loc[ df_c['task'] == \"error repair\", 'task'] = 'Feature Cleaning'\n",
    "    df_c.loc[ df_c['task'] == \"mislabel correction\", 'task'] = 'Label Cleaning'\n",
    "    df_c.loc[ df_c['task'] == \"entity matching / duplicate removal\", 'task'] = 'Entity Matching'\n",
    "    df_c.loc[ df_c['task'] == \"outliers detection\", 'task'] = 'Outlier Detection'\n",
    "    df_c.loc[ df_c['task'] == \"imputation\", 'task'] = 'Imputation'\n",
    "    df_c.loc[ df_c['task'] == \"more-than-one\", 'task'] = 'Combined'\n",
    "    df_c.loc[ df_c['task'] == \"holistic\", 'task'] = 'Holistic'\n",
    "            \n",
    "    generate_waffle_plot(df_c, 'task', file_name)\n",
    "\n",
    "plot_cleaning_activity(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
