Name,Author's affiliation,venue name,Item Type,Publication Year,Subject category,Data type,task,Abstract Note,Author,Assignee,Relevant,DOI,Compared approaches,Dataset,Error type,Limitations and shortcomings,Model,"Performance (ML-wise e.g. accuracy, etc.)","Performance (engineering-wise e.g. time, memory, etc.)",Quality measurements,In SLR,Key,Personal comments,Q1,Q1.1,Q1.2,Q1.3,Q1.4,Q1.5,Q1.6,Q1.7,Q2,Q2.1,Q2.2,Replication package link,from
Automated Detection of Label Errors in Semantic Segmentation Datasets via Deep Learning and Uncertainty Quantification,Academia,IEEE/CVF Winter Conference on Applications of Computer Vision,journalArticle,2022,ML4DC,img,mislabel correction,"‚Ä¶ labels at its boundary, which makes label error detection on pixel level a complex task. For ‚Ä¶ We study the performance of our label error detection method on synthetic image data from ‚Ä¶","Rottmann, M; Reese, M",PO,YES,,-,(1) CARLA + a bit of noise because CARLA is a simulator and it is not realistic to have 0 label error in training set (2) CityScape,-,-,"Deeplabv3+ [4] architecture with a WideResNet38 [40]
backbone, and Nvidia‚Äôs multi-scale attention approach [36]
in combination with an HRNet-OCR trunk",around 60 F1-Score on Cityscape dataset,-,"tp, fn, fp, prec, recall, f1",,WPBQEEIM,they simply used an uncertainty measure (a previous work) to send labels to be reviewed,8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,0.0,2.0,-,
READ: Aggregating Reconstruction Error into Out-of-distribution Detection,Academia,AAAI,journalArticle,2022,,,outliers detection,"Detecting out-of-distribution (OOD) samples is crucial to the safe deployment of a classifier in the real world. However, deep neural networks are known to be overconfident for abnormal data. Existing works directly design score function by mining the inconsistency from classifier for in-distribution (ID) and OOD. In this paper, we further complement this inconsistency with reconstruction error, based on the assumption that an autoencoder trained on ID data can not reconstruct OOD as well as ID. We propose a novel method, READ (Reconstruction Error Aggregated Detector), to unify inconsistencies from classifier and autoencoder. Specifically, the reconstruction error of raw pixels is transformed to latent space of classifier. We show that the transformed reconstruction error bridges the semantic gap and inherits detection performance from the original. Moreover, we propose an adjustment strategy to alleviate the overconfidence problem of autoencoder according to a fine-grained characterization of OOD data. Under two scenarios of pre-training and retraining, we respectively present two variants of our method, namely READ-MD (Mahalanobis Distance) only based on pre-trained classifier and READ-ED (Euclidean Distance) which retrains the classifier. Our methods do not require access to test time OOD data for fine-tuning hyperparameters. Finally, we demonstrate the effectiveness of the proposed methods through extensive comparisons with state-of-the-art OOD detection algorithms. On a CIFAR-10 pre-trained WideResNet, our method reduces the average FPR@95TPR by up to 9.8% compared with previous state-of-the-art.  2022, CC BY-NC-ND.","Jiang, Wenyu; Cheng, Hao; Chen, Mingcai; Feng, Shuai; Ge, Yuxin; Wang, Chongjun",PO,YES,10.48550/arXiv.2206.07459,G-ODIN-I / G-ODIN-C / G-ODIN-E,(1) SVHN (2) LSUN (3) Textures (4) Places365 (5) CIFAR-100 (6) TIN (7) LSUN (8) TIN (9) iSUN,-,-,NN and autoencoder,better,-,(1) AUROC (2) FPR@95TPR,,P78F43QV,-,12,2.0,2.0,1.0,2.0,1.0,2.0,2.0,2,0.0,2.0,https://github.com/lygjwy/READ,
Annotation Error Detection: Analyzing the Past and Present for a More Coherent Future,Academia,Computational Linguistics,journalArticle,2022,"DC4ML, ML4DC",text,mislabel correction,"‚Ä¶ Therefore, many automatic methods for annotation error detection (AED) have been devised over the years. These methods enable dataset creators and machine learning practitioners ‚Ä¶","Klie, JC; Webber, B; Gurevych, I",,YES,"https://doi.org/10.48550/arXiv.2206.02280
10.1162/coli_a_00464","1. Variation-based (Variation n-grams, Label Entropy and Weighted Discrepancy)
2. Model-based (Re-tagging, ..)
3. Training Dynamics
4. Vector Space Proximity 
5. Ensembling
6. Rule-based","1. ATIS contains transcripts of user interactions with travel inquiry systems, annotated with intents and slots.
2. IMDb contains movie reviews labeled with sentiment.
3. SST The STANFORD SENTIMENT TREEBANK is a dataset for sentiment analysis of movie reviews from Rotten Tomatoes
4. GUM The GEORGETOWN UNIVERSITY MULTILAYER CORPUS is an open source corpus annotated with several layers from the Universal Dependencies project
5. Plank POS contains Twitter posts
6. CoNLL-2003
7. Slot Inconsistencies contains documents of three domains (COMPANIES, FOREX, FLIGHTS) that were annotated via crowdsourcing","Annotation Error 
Inconsistencies","Artificial errors (used clean datasets and injected random noise) which can lead to overestimate the ability of the detector.

Focuses on errors, inconsistencies, and ambiguities related to
instance labels, but not errors concerning tokenization, sentence splitting, or missing entities.",-,"The methods that worked best overall across tasks and datasets are Borda Count (BC), Diverse Ensemble (DE), Label Aggregation (LA), and Retag (RE).

Inconsistencies seems to be more difficult to detect for most methods, especially for model-based ones.",,"for Flagger tasks (binary classification): F1, precision, recall and percentage of flagged instances
for Scorers (liklihood or ranking): AP, precision@10% and recall @10% (Precision and recall at
10% evaluate a scenario in which a scorer was applied and the first 10% with the highest score‚Äîmost likely to be incorrectly annotated‚Äîare manually corrected)",,P6D23SBS,This is more as empirical study rather than proposing new approach for cleaning data. They re-implement 18 methods for detecting potential annotation errors and evaluate them on 9 English datasets for text classification as well as token and span labeling,12,2.0,2.0,2.0,2.0,2.0,1.0,1.0,2,1.0,1.0,https://github.com/UKPLab/nessie,
Complaint-Driven Training Data Debugging at Interactive Speeds,Academia,ACM SIGMOD,conferencePaper,2022,ML4DC,tabular,mislabel correction,"Modern databases support queries that perform model inference (inference queries). Although powerful and widely used, inference queries are susceptible to incorrect results if the model is biased due to training data errors. Recently, prior work Rain proposed complaint-driven data debugging which uses user-specified errors in the output of inference queries (Complaints) to rank erroneous training examples that most likely caused the complaint. This can help users better interpret results and debug training sets. Rain combined influence analysis from the ML literature with relaxed query provenance polynomials from the DB literature to approximate the derivative of complaints w.r.t. training examples. Although effective, the runtime is O(|T|d), where T and d are the training set and model sizes, due to its reliance on the model's second order derivatives (the Hessian). On a Wide Resnet Network (WRN) model with 1.5 million parameters, it takes 1 minute to debug a complaint. We observe that most complaint debugging costs are independent of the complaint, and that modern models are overparameterized. In response, Rain++ uses precomputation techniques, based on non-trivial insights unique to data debugging, to reduce debugging latencies to a constant factor independent of model size. We also develop optimizations when the queried database is known apriori, and for standing queries over streaming databases. Combining these optimizations in Rain++ ensures interactive debugging latencies (1ms) on models with millions of parameters.  2022 ACM.","Flokas, Lampros; Wu, Weiyuan; Liu, Yejia; Wang, Jiannan; Verma, Nakul; Wu, Eugene",PO,YES,10.1145/3514221.3517849,Rain (the initial implementation they want to improve the engineering performances),"(1) MNIST, (2) Fashion-MNIST,  (3) CIFAR-10, (4) SST-2, (5) ADULT",any,-,any,"even if it uses approximations, it provides similar results (sometimes better, sometimes worse)","10 times faster online, with some pre-computation required before","precision, recall, AUC of top-k, like in the initial paper",,X8C6K9G7,"the paper is only focussed on optimization, not really data-cleaning",12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,2.0,0.0,-,
Can Foundation Models Wrangle Your Data?,Both,arxiv,journalArticle,2022,ML4DC,tabular,more-than-one,"‚Ä¶ As a proof-of-concept, we cast three data cleaning and ‚Ä¶ achieve SoTA performance on data cleaning and integration tasks, ‚Ä¶ from data science and machine learning experts who have ‚Ä¶","Narayan, A; Chami, I; Orr, L; R√©, C",PO,YES,,"‚ÄúWe compare against the SoTA methods for each task. For
entity matching, we benchmark against Ditto [46], the current SoTA
DL-based approach which finetunes BERT [28]. For data imputation,
we benchmark against IMP [60], which finetunes RoBERTa [52]
and HoloClean [71], a statistical-based SoTA data repairing engine.
Finally, for error detection, we compare against HoloClean and
HoloDetect [34], a data-augmentation based ML approach.‚Äù","entity matching: Magellan. imputation: Restaurants and Buy. error detection,: Hospital","duplicate removal, missing value, wrong value (error detection)",See paper summary for more details,GPT-3-175B parameter model (text-davinci-002) in the OpenAI API endpoint,"Entity linkage: better  half the time, data imputation and error detection: always better ",Adapting the model to the data task may take some time.,"‚ÄúFor both the error detection and entity match-
ing datasets, we evaluate performance using F1 score. For imputa-
tion, we evaluate performance using accuracy.‚Äù",YES,NIGFJBI6,-,14,2.0,2.0,2.0,2.0,2.0,2.0,2.0,4,2.0,2.0,https://github.com/HazyResearch/fm_data_tasks,
Sudowoodo: Contrastive Self-supervised Learning for Multi-purpose Data Integration and Preparation,Both,arxiv,journalArticle,2022,ML4DC,tabular,more-than-one,"‚Ä¶ For data cleaning, we combine the error detection and correction stages and evaluate the quality of the final corrections. We also conduct a case study of column semantic type ‚Ä¶","Wang, R; Li, Y; Wang, J",PO,YES,,(1) Baran+ Raha (2) Perfect error detection + Baran (3) Sudowoodo without pretraining of embeddings,(1) beers (2) hospital (3) rayyan (4) tax,any,-,a few ones. transformers are a key component.,Better than Baran with perfect ED by 2.1% on average!,-,F1-score of correct repairs,,A8N4FHG4,"The paper is not so clear, but the tool shows the author has significant ML skills.",12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,-,
Correction to: Co-active neuro-fuzzy inference system model as single imputation approach for non-monotone pattern of missing data,Academia,Neural Computing and Applications,journalArticle,2022,ML4DC,tabular,imputation,"‚Ä¶ that are used to measure the machine learning approaches precision. ‚Ä¶ on machine learning achieve in the data cleaning process. Moreover, it is considered that the machine learning-‚Ä¶","Silva-Ramirez, EL; Cabrera-S√°nchez, JF",,YES,"https://doi.org/10.1007/s00521-020-05661-5(0123456789().,-volV","ANN baseline (MLP)
Mean/Mode
Regression
Hot-deck  e6ztd","18 datasets collected from UCI Repository: Abalone, Cleveland, Contraceptive, Credit, Flag, Heart, Glass, Image-segmentation, Pima, Sonar, Wine, Yeast, Breast, Car, Hayes-Roth, Lymphography, Soybean, Tic-Tac-Toe",missing data ,,CANFIS-ART based on Neural Networks and Fuzzy logic (Fuzzy-ART algorithm)," CANFIS-ART model has the highest GCI for all datasets with both qualitative and quantitative data, but for two of the datasets with only quantitative data, Hot-deck outperforms it and for one of the qualitative only dataset Mean/Mode outperform it.
CANFIS-ART model presents the best results of all the imputation methods with the greatest difference for almost all databases, standing out mainly in the block of databases with quantitative variables.",,"**to assess the goodness of fit between observed and predicted values: 
MSE Mean of Squared Error
RMSE Root Mean of Squared Error
MAPE Mean Absolute Percentage Error
ER Error rate
R2 Coefficient of determination
GCI Global Criteria Indicator

**to measure impact of imputation: accuracy of classifiers CART, SVM and MLP applied on the original database and the predicted data sets with estimated missing values using 10-fold cross-validation then Wilcoxon signed-rank test was performed for Original and CANFIS-ART as well as between CANFIS-ART and each of the compared approaches",,SDDXK8RH,correction paper! extracted information from the original,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,,
Performance evaluation of outlier detection techniques in production timeseries: A systematic review and meta-analysis,Academia,Expert Syst. Appl.,journalArticle,2022,ML4DC,tabular,outliers detection,"Time-series data have been extensively collected and analyzed in many disciplines, such as stock market, medical diagnosis, meteorology, and oil and gas industry. Numerous data in these disciplines are sequence of observations measured as functions of time, which can be further used for different applications via analytical or data analytics techniques (e.g., to forecast future price, climate change, etc.). However, presence of outliers can cause significant uncertainties to interpretation results; hence, it is essential to remove the outliers accurately and efficiently before conducting any further analysis. A total of 17 techniques that belong to statistical, regression-based, and machine learning (ML) based categories for outlier detection in timeseries are applied to the oil and gas production data analysis. 15 of these methods are utilized for production data analysis for the first time. Two state-of-the-art and high-performance techniques are then selected for data cleaning which require minimum control and time complexity. Moreover, performances of these techniques are evaluated based on several metrics including the accuracy, precision, recall, F1 score, and Cohen's Kappa to rank the techniques. Results show that eight unsupervised algorithms outperform the rest of the methods based on the synthetic case study with known outliers. For example, accuracies of the eight shortlisted methods are in the range of 0.830.99 with a precision between 0.83 and 0.98, compared to 0.650.82 and 0.070.77 for the others. In addition, ML-based techniques perform better than statistical techniques. Our experimental results on real field data further indicate that the k-nearest neighbor (KNN) and Fulford-Blasingame methods are superior to other outlier detection frameworks for outlier detection in production data, followed by four others including density-based spatial clustering of applications with noise (DBSCAN), and angle-based outlier detection (ABOD). Even though the techniques are examined with oil and gas production data, but the same data cleaning workflow can be used to detect timeseries outliers in other disciplines.  2021","Alimohammadi, Hamzeh; Nancy Chen, Shengnan",,YES,10.1016/j.eswa.2021.116371,"ML-based:
angle-based (angle-based and fast-angle based)
Classification-based (Oneclass-SVM and Iforest0
Proximity based (Density-based spatial clustering of applications with noise, K-nearset
neighbor, Local outlier factor, Clustering-based local outlier factor, Connectivity-based
outlier factor)
Besides other statistical methods and regression-based","synthetic dataset (decline curve using traditional hyperbolic Arps model consisting 3000 days of production with a measurement frequency of 10 days) for evaluation
real oil and gas production data",outliers,"Proximity-based: (n^2) complexity level, Score sensitive to the choice of number of nearest neighbors (k), Results in low performance if data has widely variable density
Density-based: O(n^2) complexity level, Must choose two parameters (k for nearest neighbors and d for distance threshold)
Clustering-based: Requires thresholds for minimum size and distanc, Sensitive to the number of clusters chosen, Hard to associate outlier scores with objects, Outliers may affect the initial formation of clusters, Low performance in small datasets
Classififcation-based: Computationally heavy, especially when in very large datasets, Returns binary results and not the outlier score",NA (comparative study),"Synthetic dataset: The best 8 detectors out of 17 are point distribution ESD, sliding window polynomial fit, FulfordBlasingame, iForest, Fast-ABOD, DBSCAN, COF, and KNN where their  accuracies are in the range of 0.83‚Äì0.99 with a precision between 0.83 and 0.98, compared to 0.65‚Äì0.82 and 0.07‚Äì0.77 for the others
Real dataset: out of the 8 detectors above, the best two are KNN and Fulford-Blasingame while point distribution ESD and polynomial fit perform slightly lower than the rest. However, all 8 performed well with few false positives
*Fulford-Blasingame performs better than KNN only for single trends data 


","KNN outperforms all other techniques and is the top choice for outlier detection in production data based on all selection criteria (performance, computation time, difficulty of tuning hyperparameters). KNN‚Äôs overall performance stands after Fulford-Blasingame only for a single trend production data.","accuracy, precision, recall, F1 score, confusion matrix and Cohen‚Äôs Kappa to rank the techniques (for synthetic dataset)
visual inspection (for real dataset)
computation time
difficulty of tuning hyperparameters",,CF22C3TD,,14,2.0,2.0,2.0,2.0,2.0,2.0,2.0,2,0.0,2.0,,
Deep entity matching with adversarial active learning,Both,VLDB Journal,journalArticle,2022,ML4DC,tabular,entity matching / duplicate removal,"Entity matching (EM), as a fundamental task in data cleansing and integration, aims to identify the data records in databases that refer to the same real-world entity. While recent deep learning technologies significantly improve the performance of EM, they are often restrained by large-scale noisy data and insufficient labeled examples. In this paper, we present a novel EM approach based on deep neural networks and adversarial active learning. Specifically, we design a deep EM model to automatically complete missing textual values and capture both similarity and difference between records. Given that learning massive parameters in the deep model needs expensive labeling cost, we propose an adversarial active learning framework, which leverages active learning to collect a small amount of ""good"" examples and adversarial learning to augment the examples for stability enhancement. Additionally, to deal with large-scale databases, we present a dynamic blocking method that can be interactively tuned with the deep EM model. Our experiments on benchmark datasets demonstrate the superior accuracy of our approach and validate the effectiveness of all the proposed modules.  2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Huang, Jiacheng; Hu, Wei; Bao, Zhifeng; Chen, Qijin; Qu, Yuzhong",PO,YES,10.1007/s00778-022-00745-1,(1) DeepMatcher (2) Seq2SeqMatcher (3) EMTranformer (4) DItto (5) CorDEL,(1) DBLPScholar (2) DBLP-ACM (3) Amazon-Google (4) Walmart-Amazon (5) Fodors-Zagats (6) iTunes-Amazon (7) Beer,duplicates,-,"a lot. transformers, cnns, etc.",Better or equal than others on every dataset,generally lower than other approaches,precision recall f1-score,,,It could also have been named: ‚Äúhow to make something relatively simple impossible to understand‚Äù,9,2.0,2.0,1.0,2.0,0.0,1.0,1.0,4,2.0,2.0,,
Automatic Detection of Grammatical Errors in English Verbs Based on RNN Algorithm: Auxiliary Objectives for Neural Error Detection Models,Academia,Computational Intelligence and Neuroscience,journalArticle,2021,ML4DC,text,error detection/repair,"‚Ä¶ On this basis, in order to propose a more reliable and accurate method, this study explores an automatic grammar error detection method based on recurrent neural network (RNN), and ‚Ä¶","He, Y",,YES,https://doi.org/10.1155/2021/6052873,CUUI method,"conll-2014 
(selected only the sentences sentences with grammatical errors of verb form)",Grammatical,"Focus only on grammatical errors of English verbs 

",RNN,"precision 61.10%, recall rate 20.32, and F0.5 43.60. 
Compared to CUUI method, the accuracy and F0.5 are improved by 13.99% (absolute increase value =7.5) and 5.77% (absolute increase value =2.38), respectively, while recall slightly decreased (absolute decrease value ~ 1)",-,"Precision
Recall
F0.5 (assign higher weight to precision)

",,8LVSRULP,"The paper does not explain the compared approach but it seems based on Neural Networks.
The use of RNN allows to consider the sentence context which is important to detect and correct errors on verbs; thus, achieving better results than traditional NN.
Although the scope of errors is very limited (only verbs), the recall is low. ",12,2.0,2.0,1.0,2.0,1.0,2.0,2.0,2,0.0,2.0,,
Unsupervised Outlier Detection: A Meta-Learning Algorithm Based on Feature Selection,Academia,Electronics,journalArticle,2021,ML4DC,tabular,outliers detection,"‚Ä¶ unsupervised outlier detection is introduced in order to mitigate this problem. The proposed algorithm, in a fully unsupervised ‚Ä¶ ] and an invaluable step in any data cleaning process [11]. ‚Ä¶","Papastefanopoulos, V; Linardatos, P; Kotsiantis, S",,YES,https://doi.org/10.3390/electronics10182236,"Cluster-based local outlier factor
Isolation forest
K Nearest neighbours (KNN)
Histogram-base outlier detection (HBOS)
Minimum covariance determinant (MCD)
One-class SVM (OCSVM)
Principal component analysis (PCA)
Feature bagging
Angle-based outlier detector (ABOD)
Local outlier factor (LOF)","17 datasets of different meta-data (number of rows, columns and outliers), coming from a diverse pool of domains",Outlier,"For one dataset (vertebral) all the methods including the proposed method, scored almost zero precision which means none of the methods is appropriate for this dataset",Ensemble of 6 outliers detection methods,"Even though for some datasets the proposed method has lower performance than compared methods, in general it achieved the highest rank  for both ROC and precision compared to other approaches using Freidman statistical test.
However, the precision is still very low and largely varies from 0.01 to 0.91 where most of the datasets‚Äô results fall below 0.60",,"ROC and precision
Friedman‚Äôs non-parametric statistical test",,EW8PN28M,,8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,0.0,2.0,https://github.com/ML-Upatras/unsupervised-outlier-detection,
CleanML: A study for evaluating the impact of data cleaning on ml classification tasks,Academia,IEEE ICDE,conferencePaper,2021,DC4ML,tabular,holistic,"Data quality affects machine learning (ML) model performances, and data scientists spend considerable amount of time on data cleaning before model training. However, to date, there does not exist a rigorous study on how exactly cleaning affects ML - ML community usually focuses on developing ML algorithms that are robust to some particular noise types of certain distributions, while database (DB) community has been mostly studying the problem of data cleaning alone without considering how data is consumed by downstream ML analytics.We propose a CleanML study that systematically investigates the impact of data cleaning on ML classification tasks. The open-source and extensible CleanML study currently includes 14 real-world datasets with real errors, five common error types, seven different ML models, and multiple cleaning algorithms for each error type (including both commonly used algorithms in practice as well as state-of-the-art solutions in academic literature). We control the randomness in ML experiments using statistical hypothesis testing, and we also control false discovery rate in our experiments using the Benjamini-Yekutieli (BY) procedure. We analyze the results in a systematic way to derive many interesting and nontrivial observations. We also put forward multiple research directions for researchers.  2021 IEEE.","Li, Peng; Rao, Xi; Blase, Jennifer; Zhang, Yue; Chu, Xu; Zhang, Ce",PO,YES,10.1109/ICDE51399.2021.00009,-,"Citation, EEG, Marketing, Movie, Company, Restaurant, Sensor, Titanic, Credit, University, USCensus, Airbnb, BabyProduct, Clothing","missing values, outliers, duplicates, inconsistencies and mislabels",-,"(1) Logistic Regression, (2) Decision Tree, (3) Random Forest, (4) Adaboost, (5) XGBoost, (6) k-Nearest Neighbors (KNN) and (7) Naive Bayes",-,-,(1) accuracy or (2) F1 score,,DLX6V84Q,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,https://chu-data-lab.github.io/CleanML/,
TSAR: a time series assisted relabeling tool for reducing label noise,Academia,PErvasive Technologies Related to Assistive Environments Conference,journalArticle,2021,ML4DC,tabular,mislabel correction,"‚Ä¶ TSAR to remove mislabeled instances, and the method used to test TSAR on several machine learning models. In Section 4 we present the findings of the survey and label cleaning ‚Ä¶","Atkinson, G; Metsis, V",PO,YES,10.1145/3453892.3453900,Letting a human review arbitrary instances using TSAR visualisation (without prioritization),"(1) the UniMiB SHAR dataset: ‚Äúcollected from volunteers carrying commercial smartphones in their front trousers pockets‚Äù (2) UCI HAR: ‚ÄúSamples were collected at 50 Hz using commercial smartphones
mounted at the waist.‚Äù",mislabels,-,CNN,"(1) Not better than the compared approach (2) obviously, it has improved model performance; someone manually cleaned the dataset!",-,(1) Confusion matrix (2) performance of end model after cleaning,,DSB98ZH2,Their approach is not better than doing nothing (not prioritizing samples),9,2.0,2.0,1.0,2.0,0.0,1.0,1.0,2,2.0,0.0,https://github.com/imics-lab/TSAR,
A reconstruction error-based framework for label noise detection,Academia,Journal of Big Data,journalArticle,2021,ML4DC,tabular,mislabel correction,‚Ä¶ We point out that these are primarily supervised approaches [23]. Zhang and Tan [24] used ‚Ä¶ both data cleaning and classification quality. The authors follow an unsupervised approach ‚Ä¶,"Salekshahrezaee, Z; Leevy, JL; Khoshgoftaar, TM",Dima,YES,10.1186/s40537-021-00447-5,,A credit card fraud dataset by Worldline and the Universit√© Libre de Bruxelles (ULB). Kaggle: Credit Card Fraud Detection. https://www.kaggle.com/mlg-ulb/creditcardfraud.,,,"PCA, autoencoders",,,"Area Under the Receiver Operating Characteristic Curve (AUC), recall, and
False Negative Rate (FNR).",YES,94456HSI,Good for snowbowling,9,1.0,1.0,2.0,1.0,2.0,1.0,1.0,2,1.0,1.0,,
Semi-Supervised Data Cleaning with Raha and Baran.,Academia,Conference on Innovative Data Systems Research (CIDR),journalArticle,2021,ML4DC,tabular,error detection/repair,"‚Ä¶ supervised manner. In this paper, we demonstrate how both systems can be used within an end-to-end data cleaning ‚Ä¶ , both systems can optimize the data cleaning task at hand in terms ‚Ä¶","Mahdavi, M; Abedjan, Z",PO,YES,,,,,,,,,,,YAJ728SP,,0,,,,,,,,0,,,,
SPADE: A Semi-supervised Probabilistic Approach for Detecting Errors in Tables,Academia,IJCAI,conferencePaper,2021,ML4DC,tabular,error detection,"Error detection is one of the most important steps in data cleaning and usually requires extensive human interaction to ensure quality. Existing supervised methods in error detection require a significant amount of training data while unsupervised methods rely on fixed inductive biases, which are usually hard to generalize, to solve the problem. In this paper, we present SPADE, a novel semi-supervised probabilistic approach for error detection. SPADE introduces a novel probabilistic active learning model, where the system suggests examples to be labeled based on the agreements between user labels and indicative signals, which are designed to capture potential errors. SPADE uses a two-phase data augmentation process to enrich a dataset before training a deep-learning classifier to detect unlabeled errors. In our evaluation, SPADE achieves an average F1-score of 0.91 over five datasets and yields a 10% improvement compared with the state-of-the-art systems.  2021 International Joint Conferences on Artificial Intelligence. All rights reserved.","Pham, Minh; Knoblock, Craig A.; Chen, Muhao; Vu, Binh; Pujara, Jay",PO,YES,,"(1) Active learning approaches: (1.1)Raha, (1.2)Ed2 (2) Others: (2.1) dBoost, (2.2) NADEEF, (2.3)KATARA, (2.4)ActiveClean",(1) Hospital (2) Beers (3) Rayyan (4) Flights (5) Movies,any,-,"A binary classifier, open for more details",Generally better than other approaches,Compared to other active learning approaches: (1) number of labeled cells generally slightly lower and (2) time consumption slightly above,"precision, recall, F1 score",,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,https://github.com/minhptx/spade,
TabReformer: Unsupervised Representation Learning for Erroneous Data Detection,Both,ACM/IMS Transactions on Data Science,journalArticle,2021,ML4DC,tabular,error detection,"Error detection is a crucial preliminary phase in any data analytics pipeline. Existing error detection techniques typically target specific types of errors. Moreover, most of these detection models either require user-defined rules or ample hand-labeled training examples. Therefore, in this article, we present TabReformer, a model that learns bidirectional encoder representations for tabular data. The proposed model consists of two main phases. In the first phase, TabReformer follows encoder architecture with multiple self-attention layers to model the dependencies between cells and capture tuple-level representations. Also, the model utilizes a Gaussian Error Linear Unit activation function with the Masked Data Model objective to achieve deeper probabilistic understanding. In the second phase, the model parameters are fine-tuned for the task of erroneous data detection. The model applies a data augmentation module to generate more erroneous examples to represent the minority class. The experimental evaluation considers a wide range of databases with different types of errors and distributions. The empirical results show that our solution can enhance the recall values by 32.95% on average compared with state-of-the-art techniques while reducing the manual effort by up to 48.86%.  2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Nashaat, Mona; Ghosh, Aindrila; Miller, James; Quader, Shaikh",PO,YES,10.1145/3447541,(1) Holoclean (2) ED2 (3) Raha (4) ActiveClean (5) NADEEF,(1) Adult (2) Restaurants (3) Flights (4) Movies (5) Hospital (6) Beers,any (error detection),-,Transformer,Recall enhanced by 32.95% on average,Manual labeling effort reduced by 48.86%,(1) Precision (2) Recall (3) F1. Regarding the proportion of error detected - not a end model.,YES,,Pretty solid. We could say that the quality of transformer highly depends on the quality of the data augmentation module,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,https://github.com/MonaNashaat/TabReformer,
Correcting Corrupted Labels Using Mode Dropping of ACGAN,Academia,International Symposium on Medical Information and Communication Technology (ISMICT),conferencePaper,2021,"DC4ML, ML4DC",img,mislabel correction,"Machine learning often requires a large amount of training data, and the training data obtained from various sources is often of poor quality, such as a large number of corrupted labels. Researchers using machine learning often apply some data cleaning techniques to clean up corrupted data. There are two popular methods to clean corrupted data: one is to set manual cleaning rules, and the other is to use positive samples for machine learning or statistical methods. Our work proposes a data cleaning method based on ACGAN since it is difficult to manually formulate cleaning rules, and there are often no positive samples of training data too. Our work does not need to artificially add cleaning rules or positive samples, and subtly uses mode dropping of GAN to eliminate the impact of noisy labels on corrupted data so which can be converted to relatively clean synthetic training data. Mode dropping of ACGAN will naturally happens, which is originally a disadvantage that usually needs to be eliminated in GAN, we tom the disadvantage into advantage, ACGAN will ignore some non-subject features when generating data, so as to eliminate the impact of noisy labels. And we also apply our method to correct noisy labels on corrupted training data.  2021 IEEE.","Su, Jizhong; Gao, Xing; Qin, Yipeng; Guo, Shihui",PO,YES,10.1109/ISMICT51748.2021.9434911,-,(1) MNIST (2) Fashion-MNIST,-,-,ACGAN and MLP,See figures,-,(1) Accuracy of end ML model,,,"Very poorly written, we could consider even removing it from the reading list since I am still not sure what they did",7,2.0,1.0,1.0,1.0,0.0,1.0,1.0,4,2.0,2.0,-,
Mislabeled Samples Adjustment Based on Self-paced Learning Framework,Academia,International Conference on Computer and Communications (ICCC),conferencePaper,2021,"DC4ML, ML4DC",img,mislabel correction,"Human have ability to detect and correct the mistakes in life events. For instance, it could be easy for a child to find a cat image incorrectly labeled with a dog label. However, supervised learning network directly learns the mapping between features and labels even though some labels are obviously wrong. Label noise surely increases the complexity of model and undermine the model performance. Enlightened by self-paced learning (SPL) framework which can learn samples in the order of complexity, we propose one iteration-based framework called MSASL which can discriminate and correct the possibly mislabeled samples. The approach can not only achieve the data cleansing task, but also ensure the performance of the model.  2021 IEEE.","Huang, Zhongtao; Li, Xiaojuan; Deng, Lingzhu; Wei, Kaizhen; Sui, Yunfeng",PO,YES,10.1109/ICCC54389.2021.9674334,-,(1) FOD dataset (2) VOC dataset,mislabels,-,VGG-19,-,-,Percentage of mislabeled instances,NO,,very difficult to read because of poor english,7,2.0,1.0,1.0,1.0,0.0,1.0,1.0,4,2.0,2.0,-,
Deep transfer learning for multi-source entity linkage via domain adaptation,Both,arxiv,journalArticle,2021,ML4DC,tabular,entity matching / duplicate removal,"Multi-source entity linkage focuses on integrating knowledge from multiple sources by linking the records that represent the same real world entity. This is critical in high-impact applications such as data cleaning and user stitching. The state-of-the-art entity linkage pipelines mainly depend on supervised learning that requires abundant amounts of training data. However, collecting well-labeled training data becomes expensive when the data from many sources arrives incrementally over time. Moreover, the trained models can easily overfit to specific data sources, and thus fail to generalize to new sources due to significant differences in data and label distributions. To address these challenges, we present AdaMEL, a deep transfer learning framework that learns generic high-level knowledge to perform multi-source entity linkage. AdaMEL models the attribute importance that is used to match entities through an attribute-level self-attention mechanism, and leverages the massive unlabeled data from new data sources through domain adaptation to make it generic and data-source agnostic. In addition, AdaMEL is capable of incorporating an additional set of labeled data to more accurately integrate data sources with different attribute importance. Extensive experiments show that our framework achieves state-of-the-art results with 8.21% improvement on average over methods based on supervised learning. Besides, it is more stable in handling different sets of data sources in less runtime. Copyright  2021, The Authors. All rights reserved.","Jin, Di; Sisman, Bunyamin; Wei, Hao; Dong, Xin Luna; Koutra, Danai",PO,YES,,(1) TLER (2) DeepMatcher (3) EntityMatcher (4) Ditto (5) CorDel,(1) Data Integration to Knowledge Graphs (DI2KG) challenge (2) Music-1M (3) Music-3K,duplicates,-,NNs and attention mechanisms,Above in almost every case,-,PRAUC,,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
Picket: guarding against corrupted data in tabular data during learning and inference,Academia,VLDB Journal,journalArticle,2021,"DC4ML, DC4ML - intro only, ML4DC",tabular,error detection,"Data corruption is an impediment to modern machine learning deployments. Corrupted data can severely bias the learned model and can also lead to invalid inferences. We present, Picket, a simple framework to safeguard against data corruptions during both training and deployment of machine learning models over tabular data. For the training stage, Picket identifies and removes corrupted data points from the training data to avoid obtaining a biased model. For the deployment stage, Picket flags, in an online manner, corrupted query points to a trained machine learning model that due to noise will result in incorrect predictions. To detect corrupted data, Picket uses a self-supervised deep learning model for mixed-type tabular data, which we call PicketNet. To minimize the burden of deployment, learning a PicketNet model does not require any human-labeled data. Picket is designed as a plugin that can increase the robustness of any machine learning pipeline. We evaluate Picket on a diverse array of real-world data considering different corruption models that include systematic and adversarial noise during both training and testing. We show that Picket consistently safeguards against corrupted data during both training and deployment of various models ranging from SVMs to neural networks, beating a diverse array of competing methods that span from data quality validation models to robust outlier detection models.  2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.","Liu, Zifan; Zhou, Zhechun; Rekatsinas, Theodoros",PO,YES,10.1007/s00778-021-00699-w,"to many to write here, see page",(1) Wine (2) Adult (3) marketing (4) Restaurant (5) Titanic (6) HTRU2,"any error. noise considered: random, systematic, adversarial",-,Detecting errors: two-stream multi-head self-attention. Open for more details.,(1) better than other approaches to detect errors and (2) no significant improvements on the end model improvement except for when samples are poisoned,"(1) training time: scales quadratically. They suggested an approach to address this issue. (2) at inference, adds a significant overhead (more than inference), but less than a second, so not too bad for humans",(1) test accuracy of downstream model (2) F1 of error detector,,,very well written paper,13,2.0,2.0,2.0,2.0,1.0,2.0,2.0,4,2.0,2.0,,
W2WNET: A two-module probabilistic Convolutional Neural Network with embedded data cleansing functionality,Academia,Expert Syst. Appl.,journalArticle,2021,"DC4ML, ML4DC",img,mislabel correction,"Convolutional Neural Networks (CNNs) are supposed to be fed with only high-quality annotated datasets. Nonetheless, in many real-world scenarios, such high quality is very hard to obtain, and datasets may be affected by any sort of image degradation and mislabelling issues. This negatively impacts the performance of standard CNNs, both during the training and the inference phase. To address this issue we propose Wise2WipedNet (W2WNet), a new two-module Convolutional Neural Network, where a Wise module exploits Bayesian inference to identify and discard spurious images during the training, and a Wiped module takes care of the final classification, while broadcasting information on the prediction confidence at inference time. The goodness of our solution is demonstrated on a number of public benchmarks addressing different image classification tasks, as well as on a real-world case study on histological image analysis. Overall, our experiments demonstrate that W2WNet is able to identify image degradation and mislabelling issues both at training and at inference time, with positive impact on the final classification accuracy.  2021, CC BY.","Ponzio, Francesco; Macii, Enrico; Ficarra, Elisa; Di Cataldo, Santa",PO,YES,,(1) baseline (i.e. CNN without cleansing capabilities) (2) a feature data cleaning approch and (3) a label data cleaning approch (see [2] [16] in paper),(1) MNIST (2) CIFAR10 (3) CRC,any,-,CNN,-,"better than compared approach, however, not significantly",(1) % of corrupted samples removed (2) % of clean samples removed (3) end-model performance,,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
Data cleansing for deep neural networks with storage-efficient approximation of influence functions,Industry,arxiv,journalArticle,2021,"DC4ML, ML4DC",img,mislabel correction,"Identifying the influence of training data for data cleansing can improve the accuracy of deep learning. An approach with stochastic gradient descent (SGD) called SGD-influence to calculate the influence scores was proposed, but, the calculation costs are expensive. It is necessary to temporally store the parameters of the model during training phase for inference phase to calculate influence sores. In close connection with the previous method, we propose a method to reduce cache files to store the parameters in training phase for calculating inference score. We only adopt the final parameters in last epoch for influence functions calculation. In our experiments on classification, the cache size of training using MNIST dataset with our approach is 1.236 MB. On the other hand, the previous method used cache size of 1.932 GB in last epoch. It means that cache size has been reduced to 1/1,563. We also observed the accuracy improvement by data cleansing with removal of negatively influential data using our approach as well as the previous method. Moreover, our simple and general proposed method to calculate influence scores is available on our auto ML tool without programing, Neural Network Console. The source code is also available.  2021, CC BY.","Suzuki, Kenji; Kobayashi, Yoshiyuki; Narihira, Takuya",PO,YES,,"(1) The approach they improved from, (2) no data cleaning, (3) random data cleaning","MNIST, CIFAR10",-,TODO: read on influence functions,?,Minor (~0.5%),"Compared to (1), requires 1,563 times less space",Improvement of a ML model,,,,11,2.0,2.0,1.0,2.0,0.0,2.0,2.0,4,2.0,2.0,,
Chef: A cheap and fast pipeline for iteratively cleaning label uncertainties,Academia,arxiv,journalArticle,2021,"DC4ML, ML4DC",any,mislabel correction,"High-quality labels are expensive to obtain for many machine learning tasks, such as medical image classification tasks. Therefore, probabilistic (weak) labels produced by weak supervision tools are used to seed a process in which influential samples with weak labels are identified and cleaned by several human annotators to improve the model performance. To lower the overall cost and computational overhead of this process, we propose a solution called Chef (CHEap and Fast label cleaning), which consists of the following three components. First, to reduce the cost of human annotators, we use Infl, which prioritizes the most influential training samples for cleaning and provides cleaned labels to save the cost of one human annotator. Second, to accelerate the sample selector phase and the model constructor phase, we use Increm-Infl to incrementally produce influential samples, and DeltaGrad-L to incrementally update the model. Third, we redesign the typical label cleaning pipeline so that human annotators iteratively clean smaller batch of samples rather than one big batch of samples. This yields better overall model performance and enables possible early termination when the expected model performance has been achieved. Extensive experiments show that our approach gives good model prediction performance while achieving significant speed-ups. Copyright  2021, The Authors. All rights reserved.","Wu, Yinjun; Weimer, James; Davidson, Susan B.",PO,YES,,"(1) Other versions of influence functions: INFL-D, INFL-Y; (2) Active learning methods: least-confidence based and entropy based; (3) Mislabel detection approaches: O2U and TARS","(1) Fully cleaned datasets: MIMIC-CXR-JPG (MIMIC for short) [19], Chexpert [17] and Diabetic Retinopathy Detection (2) Crowdsourced datasets: Fashion 10000 (Fashion for short)1 [24], Fact Evaluation Judgement (Fact for short)2 , and Twitter sentiment analysis (Twitter for short)",mislabels,-,"models satisfying ùúá‚àístrong convexity, such as logisitc regression with regression. Note it is possible to use a pretrained deep learning method to transform the dataset if we do not train that NN.",Slightly better performances. The cleaning methods which use the labels suggested by their implementation of influence functions are surprisingly the best.,Open up for details,End-model accuracy,,,Very well written paper.,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,2.0,0.0,https://github.com/thuwuyinjun/Chef,
Active label cleaning for improved dataset quality under resource constraints,Both,Nature communications,journalArticle,2021,"DC4ML, ML4DC",img,mislabel correction,"Imperfections in data annotation, known as label noise, are detrimental to the training of machine learning models and have a confounding effect on the assessment of model performance. Nevertheless, employing experts to remove label noise by fully re-annotating large datasets is infeasible in resource-constrained settings, such as healthcare. This work advocates for a data-driven approach to prioritising samples for re-annotation-which we term ""active label cleaning"". We propose to rank instances according to estimated label correctness and labelling difficulty of each sample, and introduce a simulation framework to evaluate relabelling efficacy. Our experiments on natural images and on a new medical imaging benchmark show that cleaning noisy labels mitigates their negative impact on model training, evaluation, and selection. Crucially, the proposed approach enables correcting labels up to 4 more effectively than typical random selection in realistic conditions, making better use of experts' valuable time for improving dataset quality.  2021, CC BY.","Bernhardt, Melanie; Castro, Daniel C.; Tanno, Ryutaro; Schwaighofer, Anton; Tezcan, Kerem C.; Monteiro, Miguel; Bannur, Shruthi; Lungren, Matthew P.; Nori, Aditya; Glocker, Ben; Alvarez-Valle, Javier; Oktay, Ozan",PO,YES,,(1) random selector (2) oracle,(1) NoisyCXR (2) CIFAR10H,mislabels,"(1) ‚ÄúA limitation of data-driven approaches for handling label
noise is that they may still be able to learn from noise patterns
in the data when label errors occur in a consistent manner‚Äù
(2) ‚ÄúUnder extreme conditions where
the bounded noise rate assumption may not hold (i.e. where on
average there are more incorrect labels than correct in a given
dataset), random selection can become preferable over data-
driven approaches‚Äù",RobustML techniques with NNs,‚Äú4√ó more effectively than typical random selection in realistic conditions‚Äù,-,percentage of clean labels in the dataset,,,nature papers are so unstructured,11,2.0,2.0,1.0,2.0,2.0,1.0,1.0,4,2.0,2.0,https://github.com/microsoft/InnerEye-DeepLearning/tree/1606729c7a16e1bfeb269694314212b6e2737939/InnerEye-DataQuality,
Cost-effective variational active entity resolution,Both,IEEE ICDE,conferencePaper,2021,ML4DC,tabular,entity matching / duplicate removal,"Accurately identifying different representations of the same real-world entity is an integral part of data cleaning and many methods have been proposed to accomplish it. The challenges of this entity resolution task that demand so much research attention are often rooted in the task-specificity and user-dependence of the process. Adopting deep learning techniques has the potential to lessen these challenges. In this paper, we set out to devise an entity resolution method that builds on the robustness conferred by deep autoencoders to reduce human-involvement costs. Specifically, we reduce the cost of training deep entity resolution models by performing unsupervised representation learning. This unveils a transferability property of the resulting model that can further reduce the cost of applying the approach to new datasets by means of transfer learning. Finally, we reduce the cost of labeling training data through an active learning approach that builds on the properties conferred by the use of deep autoencoders. Empirical evaluation confirms the accomplishment of our cost-reduction desideratum, while achieving comparable effectiveness with state-of-the-art alternatives.  2021 IEEE.","Bogatu, Alex; Paton, Norman W.; Douthwaite, Mark; Davie, Stuart; Freitas, Andre",PO,YES,10.1109/ICDE51399.2021.00114,(1) DeepER (2) DeepMatcher (3) DITTO,(1) Restaurants (2) Citations 1 (3) Citations 2 (3) Cosmetics (4) Software (5) Music (6) Beer (7) Stocks (private) (8) CRM (private),duplicates,-,VAE and NN,(1) F1 (2) Precision (3) Recall,Better than compared approaches,,,,-,10,2.0,2.0,2.0,2.0,0.0,1.0,1.0,2,0.0,2.0,-,
A probabilistic database approach to autoencoder-based data cleaning,Academia,arxiv,journalArticle,2021,"DC4ML, DC4ML - intro only, ML4DC",tabular,error detection/repair,"Data quality problems are a large threat in data science. In this paper, we propose a data-cleaning autoencoder capable of near-automatic data quality improvement. It learns the structure and dependencies in the data and uses it as evidence to identify and correct doubtful values. We apply a probabilistic database approach to represent weak and strong evidence for attribute value repairs. A theoretical framework is provided, and experiments show that it can remove significant amounts of noise (i.e., data quality problems) from categorical and numeric probabilistic data. Our method does not require clean data. We do, however, show that manually cleaning a small fraction of the data significantly improves performance. Copyright  2021, The Authors. All rights reserved.","Mauritz, R.R.; Nijweide, F.P.J.; Goseling, J.; van Keulen, M.",PO,YES,,-,(1) synthetic data set crated with a Bayesian Network and (2) unnamed data sets on surgeries  conducted at a hospital and chronic pain questionnaire; available https://zenodo.org/record/5136612#.YsMBotLMLl0,,(1) The model only ingest probabilistic data; data whose attributes are distributed along many values (effectively creating a probability distribution) (2) No idea how it performs on real world data (3) They created dirty data by adding Gaussian noise to the input data. I highly doubt this is representative of real world data (they operate on categorical data).,auto-encoder,"(1) On synthetic data: too many measures to report them all, consult the paper. Notably, they achieve up to 50% quality improvement on varying amount of noise in the semi-supervised setting (2) On real world data: none. They did not had a real data set with ground truth values.",-,(1) To measure the quality of a data set: Jensen-Shannon divergence between the ground truth data and the cleaned one; (2) to measure the improvement of the cleaning algo: the quotient of the JD of dirty data and cleaned data.,,,,13,2.0,2.0,2.0,2.0,1.0,2.0,2.0,4,2.0,2.0,,
Research on Error Label Screening Method Based on Convolutional Neural Network,Academia,IEEE International Conference on Signal and Image Processing,conferencePaper,2021,"DC4ML, ML4DC",img,mislabel correction,"Supervised learning methods require a large number of labeled image data sets, but a large number of labeled image data sets are difficult to obtain in many practical applications, so it is necessary to develop weakly supervised learning methods. There is no good method for cleaning existing data sets containing false images that can simultaneously filter images with label noise and background noise. This paper presents a new method to solve the problem of image with label noise and background noise in weakly supervised learning, which can clean the wrong image with label noise and background noise. We first used the existing neural network VGG16_BN to extract the features of the image and delete the full connection layer. Then we added an error image automatic screening module to filter out the poor quality images and possible wrong images in priority. The images screened in this part include images with 40%-60% label noise and background noise. It can effectively reduce the screening range of the wrong image. We verified the effectiveness of the proposed method in the dataset of cat and dog dichoromy in the public dataset. We screened and determined that the dataset contained about 0.8% error images, and designed 200 error images to be screened by the proposed method. Finally, 186 designed error labels were screened out, which verified the effectiveness of the proposed method.  2021 IEEE.","Li, Zhengwen; Du, Wenju; Rao, Nini",,YES,10.1109/ICSIP52628.2021.9688888,"None
The paper claims they improved the accuracy of the network VGG16BN and achieved faster training, but they did not reported the VGG16BN results",images of cats and dogs from 2013 Kaggle contest ,"Label noise (mislabeled)
Background noise (poor quality)",,CNN,"screening error rate= 1.4%
screening accuracy = 53.6%
recall rate = 93%",,"* FER (screening error rate): percentage of the number of potentially wrong images (MW) screened to ALL images (ALL) contained in the
total data set
* PRE (screening accuracy): percentage true error images (TW) among all possible error images (MW) screened out
* PEC (screening recall rate): percentage of the screened true error images (TW) in all the true error images (ATW) in the data set.
",,,"Poor writing, shallow evaluation and not compared to any other approach
",8,2.0,1.0,1.0,2.0,0.0,1.0,1.0,2,1.0,1.0,,
Interactive label cleaning with example-based explanations,Academia,NeurIPS,journalArticle,2021,"DC4ML, ML4DC",any,mislabel correction,"We tackle sequential learning under label noise in applications where a human supervisor can be queried to relabel suspicious examples. Existing approaches are flawed, in that they only relabel incoming examples that look ""suspicious"" to the model. As a consequence, those mislabeled examples that elude (or don't undergo) this cleaning step end up tainting the training data and the model with no further chance of being cleaned. We propose CINCER, a novel approach that cleans both new and past data by identifying pairs of mutually incompatible examples. Whenever it detects a suspicious example, CINCER identifies a counter-example in the training set that-according to the model-is maximally incompatible with the suspicious example, and asks the annotator to relabel either or both examples, resolving this possible inconsistency. The counter-examples are chosen to be maximally incompatible, so to serve as explanations of the model's suspicion, and highly influential, so to convey as much information as possible if relabeled. CINCER achieves this by leveraging an efficient and robust approximation of influence functions based on the Fisher information matrix (FIM). Our extensive empirical evaluation shows that clarifying the reasons behind the model's suspicions by cleaning the counter-examples helps in acquiring substantially better data and models, especially when paired with our FIM approximation. Copyright  2021, The Authors. All rights reserved.","Teso, Stefano; Bontempelli, Andrea; Giunchiglia, Fausto; Passerini, Andrea",PO,YES,,"depends on the experiment, both the most relevant: (1) without counter-examples and (2) without human labeler (i.e. drop sample)",(1) Adult (2) Breast (3) 20NG (4) MNIST (5) Fashion,mislabels,only models with cross-entropy loss,(1) logistic regression (2) feed-forward neural network (3) CNN,"better, see graphs",-,(1) nb of cleaned samples vs number of considered samples (2) end model f1 score,,,-,14,2.0,2.0,2.0,2.0,2.0,2.0,2.0,4,2.0,2.0,https://github.com/abonte/cincer,
Grammatical Error Detection with Self Attention by Pairwise Training,Academia,International Joint Conference on Neural Networks (IJCNN),journalArticle,2020,ML4DC,text,error detection/repair,"‚Ä¶ -supervised with large unlabeled corpus are popular used in many NLP problems. The current state of the art models for text error detection ‚Ä¶ investigated for error detection, methods ‚Ä¶","Wang, Q; Tan, Y",,YES,10.1109/IJCNN48605.2020.9206715,"(1) sequence labelling task
(2) Bi-LSTM
(3) a model using artificially generated data 
(4) BERT-Base
(5) BERT-Large","for training: 
(i) NUCLE
(ii) Lang-8
(iii) FCE
(iv) Synthetic
for testing:
(i) CoNLL-2014
(ii) JHU FLuency-Extended GUG corpus (JFLEG)
(iii) First Certificate in English (FCE) test set",Grammatical,,6 layers of BERT model and utilizing pairwise training,The proposed model outperforms all other compared models especially after filtering poor-quality synthetic data and using pair wise training ,-,"Precision
Recall
F0.5",,D5SRAPE4,,8,1.0,1.0,2.0,1.0,0.0,2.0,1.0,2,0.0,2.0,,
GraphER: Token-centric entity resolution with graph convolutional neural networks,Academia,AAAI,conferencePaper,2020,ML4DC,tabular,entity matching / duplicate removal,"Entity resolution (ER) aims to identify entity records that refer to the same real-world entity, which is a critical problem in data cleaning and integration. Most of the existing models are attribute-centric, that is, matching entity pairs by comparing similarities of pre-aligned attributes, which require the schemas of records to be identical and are too coarse-grained to capture subtle key information within a single attribute. In this paper, we propose a novel graph-based ER model GraphER. Our model is token-centric: the final matching results are generated by directly aggregating token-level comparison features, in which both the semantic and structural information has been softly embedded into token embeddings by training an Entity Record Graph Convolutional Network (ER-GCN). To the best of our knowledge, our work is the first effort to do token-centric entity resolution with the help of GCN in entity resolution task. Extensive experiments on two real-world datasets demonstrate that our model stably outperforms state-of-the-art models. Copyright  2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.","Li, Bing; Wang, Wei; Sun, Yifang; Zhang, Linhan; Ali, Muhammad Asif; Wang, Yi",PO,YES,,"(1) Magellan (2) RNN (Mudgal et al. 2018), (3) Hybrid (Mudgal et al.
2018)",(1) Amazon-Google (2) BeerAdvo-RateBeer,duplicates,-,open up for details,above compared approches,-,precision recall f1-score of detected matches,,BTIGMQDZ,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,https://github.com/windofshadow/GraphER,
Tomographic Auto-Encoder: Unsupervised Bayesian Recovery of Corrupted Data,Both,arxiv,journalArticle,2020,ML4DC,img,error detection/repair,"‚Ä¶ Traditional data cleaning methods rely on some degree of ‚Ä¶ in recent years, makes any supervised algorithm impractical in the ‚Ä¶ to perform automated data cleaning and recovery without ‚Ä¶","Tonolini, F; Moreno, PG; Damianou, A; ...",PO,YES,,(1) missing value imputation VAE (MVAE) (2) missing values importance weighted auto encoder (MIWAE),"MNIST (image), Fashion-MNIST (image), UCI HAR (tabular), NYU depth maps (image)","noise, missing value",-,modified VAE (they named it TAE),generally better,-,(1) peak signal to noise ratio (PSNR) for image reconstruction quality and (2) end model accuracy,,34QB87SI,"(1) very mathy, I did not fully understood their approach (2) they have data leaks in their experiments (3) I am not sure whether we should include it in our SLR, if yes, perform snowballing",11,2.0,2.0,1.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
Training-ValueNet: A new approach for label cleaning on weakly-supervised datasets,Academia,IEEE International Conference on Development and Learning and Epigenetic Robotics (ICDL-EpiRob),conferencePaper,2020,ML4DC,img,mislabel correction,‚Ä¶ supervised label cleaning and outlier detection. Let us now briefly examine each approach in turn. Semi-supervised label cleaning ‚Ä¶ partial automation of the label cleaning process. The ‚Ä¶,"Smyth, L",PO,YES,,(1) semi-supervised methods and (2) unsupervised methods. See paper for the full list.,(1) Clothing 1M (2) Aircraft-7 ,mislabels,-,Training ValueNet is a MLP with one hidden layer. They use different CNNs for feature extraction. Read paper for more details.,Better than unsupervised methods but worse than semi-supervised ones. Their method is an unsupervised one (it can be semi-supervised if we provide a clean label set),-,(1) accuracy of error detection (2) improvement of the end model,,U3FCC77F,"Clear, short and sweet. the perfect paper.",12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
A Comprehensive Benchmark Framework for Active Learning Methods in Entity Matching,Both,ACM SIGMOD,journalArticle,2020,ML4DC,tabular,entity matching / duplicate removal,"Entity Matching (EM) is a core data cleaning task, aiming to identify different mentions of the same real-world entity. Active learning is one way to address the challenge of scarce labeled data in practice, by dynamically collecting the necessary examples to be labeled by an Oracle and refining the learned model (classifier) upon them. In this paper, we build a unified active learning benchmark framework for EM that allows users to easily combine different learning algorithms with applicable example selection algorithms. The goal of the framework is to enable concrete guidelines for practitioners as to what active learning combinations will work well for EM. Towards this, we perform comprehensive experiments on publicly available EM datasets from product and publication domains to evaluate active learning methods, using a variety of metrics including EM quality, #labels and example selection latencies. Our most surprising result finds that active learning with fewer labels can learn a classifier of comparable quality as supervised learning. In fact, for several of the datasets, we show that there is an active learning combination that beats the state-of-the-art supervised learning result. Our framework also includes novel optimizations that improve the quality of the learned model by roughly 9% in terms of F1-score and reduce example selection latencies by up to 10 without affecting the quality of the model. Copyright  2020, The Authors. All rights reserved.","Meduri, Vamsi; Popa, Lucian; Sen, Prithviraj; Sarwat, Mohamed",PO,YES,,Open page,(1) Abt-Buy (2) Amazon-GoogleProducts (3) DBLP-ACM (4) DBLP-Scholar (5) Cora (6) Walmart-Amazon (7) Amazon-BestBuy (8) BeerAdvocate-RateBeer (9) BuyBuyBaby-BabiesRUs,-,-,-,-,-,See paper. F1-score,,78EGLADL,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,-,
A flexible outlier detector based on a topology given by graph communities,Academia,Big Data Research,journalArticle,2020,DC4ML,tabular,outliers detection,"Outlier, or anomaly, detection is essential for optimal performance of machine learning methods and statistical predictive models. It is not just a technical step in a data cleaning process but a key topic in many fields such as fraudulent document detection, in medical applications and assisted diagnosis systems or detecting security threats. In contrast to population-based methods, neighborhood based local approaches are simple flexible methods that have the potential to perform well in small sample size unbalanced problems. However, a main concern of local approaches is the impact that the computation of each sample neighborhood has on the method performance. Most approaches use a distance in the feature space to define a single neighborhood that requires careful selection of several parameters. This work presents a local approach based on a local measure of the heterogeneity of sample labels in the feature space considered as a topological manifold. Topology is computed using the communities of a weighted graph codifying mutual nearest neighbors in the feature space. This way, we provide with a set of multiple neighborhoods able to describe the structure of complex spaces without parameter fine tuning. The extensive experiments on real-world data sets show that our approach overall outperforms, both, local and global strategies in multi and single view settings. Copyright  2020, The Authors. All rights reserved.","Ramos Terrades, O.; Berenguel, A.; Gil, D.",PO,YES,,"LOF, LOCI, KNN, IF, APS, GMM, SO-GAAL",(1) Iris (2) BCW (3) Ionosphere (4) Letter recognition,-,-,SVM,top 2 approaches in every dataset,-,AUC of detected outliers,,4ECQI72R,a preprint,11,2.0,2.0,1.0,2.0,0.0,2.0,2.0,2,1.0,1.0,-,
Identifying label noise in time-series datasets,Academia,UBICOMP,journalArticle,2020,,tabular,mislabel correction,,"Atkinson, Gentry; Metsis, Vangelis",PO,YES,,-,(1) Sussex-HuaWei 1 (2) Sussex-HuaWei 2 (3) UniMiB SHAR 1 (4) UniMib SHAR 2,mislabel,-,CNN,-,-,Performance of the end-model,,3B2EN9RI,-,7,2.0,2.0,0.0,1.0,0.0,1.0,1.0,4,2.0,2.0,-,
Time series data cleaning: From anomaly detection to anomaly repairing (technical report),Academia,arxiv,journalArticle,2020,"DC4ML, ML4DC",tabular,error detection/repair,"Errors are prevalent in time series data, such as GPS trajectories or sensor readings. Existing methods focus more on anomaly detection but not on repairing the detected anomalies. By simply filtering out the dirty data via anomaly detection, applications could still be unreliable over the incomplete time series. Instead of simply discarding anomalies, we propose to (iteratively) repair them in time series data, by creatively bonding the beauty of temporal nature in anomaly detection with the widely considered minimum change principle in data repairing. Our major contributions include: (1) a novel framework of iterative minimum repairing (IMR) over time series data, (2) explicit analysis on convergence of the proposed iterative minimum repairing, and (3) efficient estimation of parameters in each iteration. Remarkably, with incremental computation, we reduce the complexity of parameter estimation from O(n) to O(1). Experiments on real datasets demonstrate the superiority of our proposal compared to the state-of-the-art approaches. In particular, we show that (the proposed) repairing indeed improves the time series classification application. Copyright  2020, The Authors. All rights reserved.","Zhang, Aoqian; Song, Shaoxu; Wang, Jianmin; Yu, Philip S.",PO,YES,,(1) AR (2) ARX (3) ARIMA (4) Tsay models (5) EWMA (6) SCREEN,(1) GPS dataset (the collection of the position of a person walking on a known path) (2) Intel Lab Data (a collection of measurements taken from 54 sensors),"Noise on scalar data, missing values",(1) They tested their approach only in scenario without features (only the timestamp),Linear regression,Generally superior to compared approaches,Higher than other techniques because of the iterative approach,RMS (we are doing regression rather than classification),,,,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,2.0,0.0,,
PClean: Bayesian data cleaning at scale with domain-specific probabilistic programming,Academia,Proceedings of Machine Learning Research,journalArticle,2020,ML4DC,tabular,error repair,"Data cleaning is naturally framed as probabilistic inference in a generative model, combining a prior distribution over ground-truth databases with a likelihood that models the noisy channel by which the data are filtered, corrupted, and joined to yield incomplete, dirty, and denormalized datasets. Based on this view, we present PClean, a unified generative modeling architecture for cleaning and normalizing dirty data in diverse domains. Given an unclean dataset and a probabilistic program encoding relevant domain knowledge, PClean learns a structured representation of the data as a relational database of interrelated objects, and uses this latent structure to impute missing values, identify duplicates, detect errors, and propose corrections in the original data table. PClean makes three modeling and inference contributions: (i) a domain-general non-parametric generative model of relational data, for inferring latent objects and their network of latent connections; (ii) a domain-specific probabilistic programming language, for encoding domain knowledge specific to each dataset being cleaned; and (iii) a domain-general inference engine that adapts to each PClean program by constructing data-driven proposals used in sequential Monte Carlo and particle Gibbs. We show empirically that short ( 50-line) PClean programs deliver higher accuracy than state-of-the-art data cleaning systems based on machine learning and weighted logic; that PCleans inference algorithm is faster than generic particle Gibbs inference for probabilistic programs; and that PClean scales to large real-world datasets with millions of rows. Copyright  2020, The Authors. All rights reserved.","Lew, Alexander K.; Agrawal, Monica; Sontag, David; Mansinghka, Vikash",Amin,YES,,"Holoclean, NADEEF","(1) Hospital (2) Flights (3) Rents, a synthetic dataset based on census statistics",-,(1) The user has to learn their programming language which is an extension of Julia and ,SMC and MCMC,Equal or better than other approaches,Generally equal or better than other approaches,Number of errors detected (F1 score),,,Though read,9,2.0,1.0,1.0,2.0,0.0,1.0,2.0,2,1.0,1.0,,
Batchwise probabilistic incremental data cleaning,Academia,arxiv,journalArticle,2020,"DC4ML, DC4ML - intro only, ML4DC",tabular,error detection/repair,"Lack of data and data quality issues are among the main bottlenecks that prevent further artificial intelligence adoption within many organizations, pushing data scientists to spend most of their time cleaning data before being able to answer analytical questions. Hence, there is a need for more effective and efficient data cleaning solutions, which, not surprisingly, is rife with theoretical and engineering problems. This report addresses the problem of performing holistic data cleaning incrementally, given a fixed rule set and an evolving categorical relational dataset acquired in sequential batches. To the best of our knowledge, our contributions compose the first incremental framework that cleans data (i) independently of user interventions, (ii) without requiring knowledge about the incoming dataset, such as the number of classes per attribute, and (iii) holistically, enabling multiple error types to be repaired simultaneously, and thus avoiding conflicting repairs. Extensive experiments show that our approach outperforms the competitors with respect to repair quality, execution time, and memory consumption. Copyright  2020, The Authors. All rights reserved.","Oliveira, Paulo H.; Traina-Jr, Caetano; Kaster, Daniel S.; Ilyas, Ihab F.",PO,YES,,(1) HC-Sep and (2) HC-Acc; open page for more details,(1) Hospital (2) Food (3) Soccer (corrupted with BART),See Holoclean: Holistic data repairs with probabilistic inference (https://www.notion.so/Holoclean-Holistic-data-repairs-with-probabilistic-inference-42f4c4def823491cb45e6c202ea38b43?pvs=21) ,-,See Holoclean: Holistic data repairs with probabilistic inference (https://www.notion.so/Holoclean-Holistic-data-repairs-with-probabilistic-inference-42f4c4def823491cb45e6c202ea38b43?pvs=21) ,iHC-Re performs almost the same (sometimes even better) than the approach that cleans the whole dataset at once (HC-Acc),Comparable to the approach that considers each batch separately (HC-Sep) in terms of memory and time.,(1) Number of remaining errors (as new batch of data come) (2) Total time (as new batches of data come) (3) Memory consumption (is it in total?),,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,2.0,0.0,-,
Does data cleaning improve heart disease prediction?,Academia,Procedia Comput. Sci.,journalArticle,2020,"DC4ML, ML4DC",tabular,entity matching / duplicate removal,"Data quality has become an important issue. This issue becomes more and more important in medicine area, where the need for effective decision making is high. In this context, the need for data cleaning to improve data quality is becoming crucial. Duplicate records elimination is a challenging data cleansing task. In this paper, we present a duplicate records elimination approach to improve the quality of data. We propose a deep learning-based approach for duplicate records detection using a sentence embeddings model. Also, we propose an algorithm for duplicated records correction. Then, we apply the proposed duplicate records elimination approach to analyse the effect of data cleaning on the quality of decisions. We evaluate our proposal on heart disease problem using Cleveland heart disease dataset. Experiments show that the classification performance improves upon the application of the duplicate records elimination approach on datasets compared to that of datasets with duplicate records.  2020 The Authors. Published by Elsevier B.V.","Lattar, Hafsa; Salem, Aicha Ben; Ghezala, Henda Hajjami Ben",PO,YES,10.1016/j.procs.2020.09.109,-,Cleveland heart disease dataset (with noise artificially inserted),duplicates,-,SVM,-,-,(1) F1-score (2) precision (3) recall (4) accuracy,,,-,9,2.0,2.0,0.0,1.0,0.0,2.0,2.0,4,2.0,2.0,-,
CORDEL: A contrastive deep learning approach for entity linkage,Both,IEEE International Conference on Data Mining (ICDM),journalArticle,2020,ML4DC,tabular,entity matching / duplicate removal,"Entity linkage (EL) is a critical problem in data cleaning and integration. In the past several decades, EL has typically been done by rule-based systems or traditional machine learning models with hand-curated features, both of which heavily depend on manual human inputs. With the ever-increasing growth of new data, deep learning (DL) based approaches have been proposed to alleviate the high cost of EL associated with the traditional models. Existing exploration of DL models for EL strictly follows the well-known twin-network architecture. However, we argue that the twin-network architecture is sub-optimal to EL, leading to inherent drawbacks of existing models. In order to address the drawbacks, we propose a novel and generic contrastive DL framework for EL. The proposed framework is able to capture both syntactic and semantic matching signals and pays attention to subtle but critical differences. Based on the framework, we develop a contrastive DL approach for EL, called CORDEL, with three powerful variants. We evaluate CORDEL with extensive experiments conducted on both public benchmark datasets and a real-world dataset. CORDEL outperforms previous state-of-the-art models by 5.2% on public benchmark datasets. Moreover, CORDEL yields a 2.4% improvement over the current best DL model on the real-world dataset, while reducing the number of training parameters by 97.6%. Copyright  2020, The Authors. All rights reserved.","Wang, Zhengyang; Sisman, Bunyamin; Wei, Hao; Dong, Xin Luna; Ji, Shuiwang",PO,YES,,(1) non-DL baseline is Magellan (2) DL baseline is Deep Matcher (with 4 versions),"(1) Tabular: BeerAdvo-RateBeer, iTunes-Amazon1, Fodors-Zagats, DBLP-ACM, DBLP-Scholar, Amazon-Google, Walmart-Amazon (2) Textual: Abt-Buy (3) Dirty tabular: iTunes-Amazon, DBLP-ACM, DBLP-Scholar, Walmart-Amazon (4) Real-world (tabular): Amazon-Wikipedia",duplicates,-,"MLP, Attention, pre-trained embeddings","Generally significantly better, open up for more details",requires less parameters than DL baseline,"(1) Area Under the Precision-Recall Curve (PRAUC),  (2) Recall when
Precision=95% (R@P=95%),  (3) F1 score",,,pretty good paper,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
A novel data repairing approach based on constraints and ensemble learning,Academia,Expert Syst. Appl.,journalArticle,2020,ML4DC,tabular,error detection/repair,"Data repairing is an important task in data mining. This paper proposes a novel data repairing approach based on a combination of constraints and ensemble learning. At first, functional dependencies (FDs) are used as constraints to identify inconsistent records. For each FD, all repeated values in the correct records are discovered. After that, noisy attributes in erroneous records are detected using correct records and the repeated values. To correct the detected noises, a supervised ensemble learning model is constructed for each attribute. The ensemble model consists of a Bayes classifier, a decision tree, and a MultiLayer Perceptron (MLP). A majority of votes is used as the combination strategy in the ensemble learning model. The proposed approach automatically repairs data without any user interaction. Moreover, the proposed method can detect more than one noise in a record. Experimental results show that our approach outperforms similar repairing algorithms (HoloClean and KATARA) in both terms of precision and recall.  2020 Elsevier Ltd","Ataeyan, Mahdieh; Daneshpour, Negin",PO,YES,10.1016/j.eswa.2020.113511,(1) Holoclean (2) Katara,(1) Adult (2) Protein (3) Car (4) Wine,-,-,"(1) decision tree, (2) a Bayes classifier, (3) and an MLP.",Comparable to other approaches,-,(1) precision (2) recall (3) and other variations of TP and TN,,,"poor English, while the approach is really simple, they managed to make it convuluted",8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,0.0,2.0,-,
Advancing Data Curation with Metadata and Statistical Relational Learning / Verbesserung der Datenvorbereitung Mit Metadaten und Statistisch-Relationalem Lernen,Academia,proquest,thesis,2020,ML4DC,tabular,error detection,"The foundation of every data science project depends on clean data because the quality
of the data determines the quality of the insights derived from data by using machine
learning or analytics. In this dissertation, we tackle the problem of data cleaning and
provide three approaches to advance data error detection and repair: (1) We establish a
mapping that reflects the connection between data quality issues and extractable dataset‚Äôs
metadata, and propose this mapping as a guideline for rapid prototyping of an error
detection strategy; (2) We introduce two holistic approaches for effectively combining
different error detection strategies to increase the efficacy of error detection. Our methods
are based on state-of-the-art ensemble learning algorithms and incorporate the metadata
of the dataset; and (3) We propose an approach for addressing data quality issues by
formulating a set of data cleaning rules without the manual specification of the rules
execution order. The concepts of statistical relational learning and probabilistic inference
provide the foundation for our method. We use the Markov logic formalism, because it
declaratively models data quality rules as first-order logic sentences. Markov logic allows
the usage of probabilistic joint inference over data cleaning rules to detect data errors and
suggest a repair.","Visengeriyeva, Larysa",PO,YES,,,,,,,,,,,,,0,,,,,,,,0,,,,
Nearest neighbor classifiers over incomplete information: From certain answers to certain predictions,Both,arxiv,journalArticle,2020,"DC4ML, ML4DC",any,error detection/repair,"Machine learning (ML) applications have been thriving recently, largely attributed to the increasing availability of data. However, inconsistency and incomplete information are ubiquitous in real-world datasets, and their impact on ML applications remains elusive. In this paper, we present a formal study of this impact by extending the notion of Certain Answers for Codd tables, which has been explored by the database research community for decades, into the field of machine learning. Specifically, we focus on classification problems and propose the notion of ""Certain Predictions"" (CP) - a test data example can be certainly predicted (CP'ed) if all possible classifiers trained on top of all possible worlds induced by the incompleteness of data would yield the same prediction. We study two fundamental CP queries: (Q1) checking query that determines whether a data example can be CP'ed; and (Q2) counting query that computes the number of classifiers that support a particular prediction (i.e., label). Given that general solutions to CP queries are, not surprisingly, hard without assumption over the type of classifier, we further present a case study in the context of nearest neighbor (NN) classifiers, where efficient solutions to CP queries can be developed - we show that it is possible to answer both queries in linear or polynomial time over exponentially many possible worlds. We demonstrate one example use case of CP in the important application of ""data cleaning for machine learning (DC for ML)."" We show that our proposed CPClean approach built based on CP can often significantly outperform existing techniques in terms of classification accuracy with mild manual cleaning effort. Copyright  2020, The Authors. All rights reserved.","Karla, Bojan; Li, Peng; Wu, Renzhi; Gurel, Nezihe Merve; Chu, Xu; Wu, Wentao; Zhang, Ce",PO,YES,,"(1) ML model trained on the ground truth (cleaned dataset, serves as an upper bound), (2) Default cleaning (i.e. simple imputation techniques) (3) HoloClean (4) BoostClean (5) random manual cleaning","https://sites.google.com/site/anhaidgroup/projects/data (real errors, they manually created a cleaned version of it by imputing values by searching for the true values), Supreme, Bank, Puma (created a dirty version by removing values). Find the reference of the data sets in the paper.",Missing value,(1) limited to knn classifier and (2) finding the possible worlds is expensive,The model used to guide the cleaning process is knn,Managed to completely close the gap in most experiments.,"(1) To achieve 100% gap closed, not all the samples had to be cleaned. However, they managed between 40% to 100% gap closed with a limited budget of 20% of the instances manually cleaned. (2) Open this page on Notion for a formula of the time complexity of the algorithm to find next samples to clean","‚ÄúGap closed‚Äù: % of total improvement done, compared with the lower performance bound (i.e. compared approach #2) and the upper performance bound (i.e. compared approach #1)",,,,12,2.0,2.0,1.0,2.0,2.0,1.0,2.0,3,2.0,1.0,,
Similarity-learning information-fusion schemes for missing data imputation,Academia,Knowledge-Based Systems,journalArticle,2020,ML4DC,tabular,imputation,"Missing data imputation is a very important data cleaning task for machine learning and data mining with incomplete data. This paper proposes two novel methods for missing data imputation, named kEMI and kEMI+, that are based on the k-Nearest Neighbours algorithm for pre-imputation and the Expectation-Maximization algorithm for posterior-imputation. The former is a local search mechanism that aims to automatically find the best value for k and the latter makes use of the best k nearest neighbours to estimate missing scores by learning global similarities. kEMI+ makes use of a novel information fusion mechanism. It fuses top estimations through the Dempster-Shafer fusion module to obtain the final estimation. They handle both numerical and categorical features. The performance of the proposed imputation techniques are evaluated by applying them on twenty one publicly available datasets with different missingness and ratios, and, then, compared with other state-of-the-art missing data imputation techniques in terms of standard evaluation measures such as the normalized root mean square difference and the absolute error. The attained results indicate the effectiveness of the proposed novel missing data imputation techniques. [All rights reserved Elsevier].","Razavi-Far, R.; Boyuan Cheng; Saif, M.; Ahmadi, M.",,YES,10.1016/j.knosys.2019.06.013,"EMI 
DMI 
kDMI 
kNNI 
LCSR 
LRMC 
CLRMC
CLRMC-EN","21 publicly available dataset: 
BCW, BUPA, Dermatology, Glass, Ionosphere, Iris, Letter, PID, Sheart, Sonar, Spam, Wine, Yeast, Zoo, Car, House Votes, Promoters, SPECT, Splice, TTTEG",missing data (categorical and numerical imputation),although both KEMI and KEMI+ are more accurate compared to other methods their complexity is higher. Also KEMI+ is less scalable than KEMI,"two models:
KEMI based on KNNI and Expectation‚ÄìMaximization based Imputation (EMI) algorithms
KEMI+ similar to KEMI but integrates an information fusion module (Dempster‚ÄìShafer Fusion (DSF))","According to Friedman rank test which compares NRMS: KEMI+ and KEMI have the first and second ranks and significantly outperforms the other methods; however, the difference in performance between KEMI and KEMI+ is not significant.","*complexity of KEMI and KEMI+ is similar :
O(m2k + m2d + mk2d‚Ä≤ + mk3 + kd2 + d3)
assuming d (attributes) << m (records) ‚Üí O(m2k + mk3) 
assuming d << m AND k (best k values) << m ‚Üí O(m2)
* has higher complexity compared to other methods especially KMI and KDMI (O(m)) and LRMC (O(tm)
*KEMI is more scalable than KEMI+ in terms of run time with increased dataset size (‚â•25k) or dimensions (‚â•200)","normalized root mean square difference (NRMS) 
absolute error (AE)
Friedman rank test",,,,12,2.0,2.0,2.0,2.0,1.0,1.0,2.0,2,0.0,2.0,,
Relational pretrained transformers towards democratizing data preparation [Vision] [arXiv],Academia,arxiv,journalArticle,2020,"DC4ML, DC4ML - intro only, ML4DC",tabular,error repair,"Can AI help automate human-easy but computer-hard data preparation tasks (for example, data cleaning, data integration, and information extraction), which currently heavily involve data scientists, practitioners, and crowd workers? We envision that human-easy data preparation for relational data can be automated. To this end, we first identify the desiderata for computers to achieve near-human intelligence for data preparation: computers need a deep-learning architecture (or model) that can read and understand millions of tables; computers require unsupervised learning to perform self-learning without labeled data, and can gain knowledge from existing tasks and previous experience; and computers desire few-shot learn-ing that can adjust to new tasks with a few examples. Our proposal is called Relational Pretrained Transformers (RPTs), a general framework for various data preparation tasks, which typically consists of the following models/methods: (1) transformer, a general and powerful deep-learning model, that can read tables/texts/images;(2) masked language model for self-learning and collaborative train-ing for transferring knowledge and experience; and (3) pattern-exploiting training that better interprets a task from a few examples.We further present concrete RPT architectures for three classical data preparation tasks, namely data cleaning, entity resolution, and information extraction. We demonstrate RPTs with some initial yet promising results. Last but not least, we identify activities that will unleash a series of research opportunities to push forward the field of data preparation.","Nan Tang; Ju Fan; Fangyi Li; Jianhong Tu; Xiaoyong Du; Guoliang Li; Madden, S.; Ouzzani, M.",PO,YES,,,"Abt-Buy, Walmart-Amazon, Amazon-Google",,"(1) They trained and tested on different data sets. Normally, it is not a good practice to test on a different dataset, but here, in NLP, it is common. If they are able to accurately transform tables in natural language, it would be fine. (2) Numeric values are mapped to unknown token. (3) Transformers have a max sequence length in their input, which limits the input size of a table entry.",They call their model RPT. It is composed of a encoder and a decoder (similar to BERT(encoder) + GPT(decoder)). RPT uses BART weights (pre-training).,,not mentioned,"none for data cleaning (they just said ‚Äúthe cleaning propositions make sense‚Äù) and for entity linkage, they achive the best results compared to SOTA (.71 F1 score)",,,,7,1.0,1.0,1.0,1.0,1.0,1.0,1.0,4,2.0,2.0,,
Unknown Class Label Cleaning for Learning with Open-Set Noisy Labels,Academia,IEEE International Conference on Image Processing (ICIP),conferencePaper,2020,"DC4ML, ML4DC",img,mislabel correction,"Deep neural networks (DNNs) trained on large-scale annotated datasets have achieved impressive results in the area of image classification. Many large-scale datasets have been collected from websites; however, such data are inevitably corrupted with noise. In this study, we researched the open-set noisy label problem, where some outliers are contained in a dataset and annotated through a noisy label but do not belong to any class of training data. To address this problem, we propose a novel unknown class label cleaning framework for the training of DNNs with open-set noisy labels. In addition to general image classification, we also estimate the probability of an input being from an unknown class by assigning a pseudo unknown label to all of the data and correct these labels through an alternating update of the network parameters and labels. The results of experiments conducted on the noisy CIFAR-10 datasets demonstrate that our approach can robustly train DNNs with a high proportion of noisy labels.  2020 IEEE.","Yu, Qing; Aizawa, Kiyoharu",,YES,10.1109/ICIP40778.2020.9190652,"(1) symmetric cross entropy (SCE)
(2) generalized cross entropy (GCE) 
(3) co-teaching+",CIFAR-10,mislabel (outliers),,ResNet-32,highest accuracy (76.45 to 83.15) among compared models and lowest error detection almost in all cases of noise (0.20 to 20.39) except for the added noise (CIFAR-100) where co-teaching+ achieved lower error detection (13.95 vs. 15.78 for that particular noise),-,Mean classification accuracy and detection error,,,Good simple idea but poor evaluation,11,2.0,2.0,2.0,2.0,0.0,1.0,2.0,4,2.0,2.0,,
Data Cleaning of Irrelevant Images Based on Transfer Learning,Academia,International Conference on Intelligent Computing Automation and Systems (ICICAS),conferencePaper,2020,"DC4ML, ML4DC",img,mislabel correction,"Considering that there are many irrelevant images in the image data set crawled from the Internet, this paper proposes a new method of cleaning irrelevant images data based on transfer learning. The method consists of two steps that can be done iteratively. Firstly, we use transfer learning to train and get a better image classifier, which makes the single image's recognition effect more accurate. Besides, according to the number of different types of images in the data set, we calculate and determine the threshold value of the image of the minority class and then clean them accordingly. We find that our method can improve the test accuracy of the original data set on VGG16, VGG19, and Inception v3 from 37.41%, 40.62%, and 49.37% to 50.07%, 53.89%, and 65.16%, respectively. Since transfer learning has a good application effect on irrelevant images cleaning, we can use it to create data sets in deep learning.  2020 IEEE.","Liu, Dongzhen; Meng, Yanli; Wang, Lianming",PO,YES,10.1109/ICICAS51530.2020.00099,-,"Custom data set based on ""Atlas of Primary Colors of Marine Fishes in the South China Sea 1"" and ""Atlas of Primary Colors of Marine Fishes in the
South China Sea 2‚Äù ",,"(1) They removed minority classes. Obviously they will have a better accuracy, since they removed the most challenging classes. (2) They remove classes. Maybe we do not want to remove these. (3) The performance of their approach to remove samples of poor quality depends heavily on the quality of the CNN to recognize images.","VGG19, InceptionV3, VGG16",around 15% accuracy increase for the 3 models,not mentioned,Accuracy improved between 12-15% depending on the model used,,,,7,1.0,1.0,1.0,2.0,0.0,1.0,1.0,4,2.0,2.0,,
Ed2: A case for active learning in error detection,Academia,ACM CIKM,conferencePaper,2019,ML4DC,tabular,error detection,"State-of-the-art approaches formulate error detection as a semi-supervised classification problem. Recent research suggests that active learning is insufficiently effective for error detection and proposes the usage of neural networks and data augmentation to reduce the number of these user-provided labels. However, we can show that using the appropriate active learning strategy, it is possible to outperform the more complex models that rely on data augmentation. To this end, we propose a multi-classifier approach with two-stage sampling for active learning. This intuitive and neat sampling method chooses the most promising cells across rows and columns for labeling. On three datasets, ED2 achieves state-of-the-art detection accuracy while for large datasets, the required number of user labels is lower by one order of magnitude compared to the state of the art.  2019 Association for Computing Machinery.","Neutatz, Felix; Mahdavi, Mohammad; Abedjan, Ziawasch",PO,YES,,(1) NADEEF (2) dBoost (3) KATARA (4) ActiveClean (5) BoostClean (6) Metadata-driven error detection,(1) Flights (2) Movies (3) Beers (4) Address,any,-,XGBoost,"Not the fastest to learn, but on the long run, it outperforms compared approaches","12 minutes on the largest dataset, 1% of instance must be labeled to have good performances",Precision recall and f1-score of detected errors,YES,,,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,https://github.com/BigDaMa/ExampleDrivenErrorDetection,Ed2: A case for active learning in error detection (https://www.notion.so/Ed2-A-case-for-active-learning-in-error-detection-38a5012844dd49639218a8fc200ed901?pvs=21) 
Automatic grammatical error detection of non-native spoken learner English,Academia,ICASSP,conferencePaper,2019,ML4DC,text,error detection/repair,‚Ä¶ The grammatical error detection (GED) system chosen for this work is a state-of-the-art bidirectional recurrrent neural network based framework2 proposed for detecting all kinds of ‚Ä¶,"Knill, KM; Gales, MJF; Manakul, PP; ...",,YES,https://doi.org/10.1109/ICASSP.2019.8683080,"(1) error detection for written text (FCE) vs. spoken (NICT-JLE & BULTAS)
(2) error detection using manual transcriptions vs. Automatic transcription (ASR) ","(1) CLC FCE: The Cambridge Learner Corpus (CLC) of ESOL First Certificate in English (written text used to train the baseline model and then fine tuned for the other 2 spoken corpus)
(2) BULATS: The spoken BULATS Business English assessment
test
(3) NICT-JLE: The NICT Japanese Learner English (JLE) Corpus


",Grammatical ,Unavailability of large corpus for speech training data ,"BLSTM 
","(1) The performance of baseline GED for spoken is less than baseline GED for written text which achieves (P=69.9, R=33.9,F0.5=57.6). However, when GED is fine-tuned using the spoken dataset, the performance improved (for BULTAS: P=66.7, R=33.8,F0.5=55.8; for NICT-JLE: P=60.6, R=28.9, F0.5=49.7) and be close to the level of GED for written text.

(2) Baseline GED for manual transcription has higher performance (P=52.4, R=27.0, F0.5=44.1) compared to baseline GED for ASR transcription (P=28.1, R=22.0, F0.5=26.6)",-,"Precision
Recall
F0.5 (assign higher weight to precision)",,P27VEW2B,"Unlike others techniques applied to written text, the detection of grammatical errors in this paper is applied to speech (transcript provided either by ASR or manually).
They use the existing state-of-the-art GED model.
Using ASR transcriptions is more challenging to the GED model because ASR-related errors increase with grammatical errors and disfluencies leading to degrade GED performance.",13,2.0,2.0,2.0,2.0,1.0,2.0,2.0,2,0.0,2.0,,
HoloDetect: Few-Shot Learning for Error Detection [arXiv],Academia,ACM SIGMOD,journalArticle,2019,ML4DC,tabular,error detection,"We introduce a few-shot learning framework for error detection. We show that data augmentation (a form of weak supervision) is key to training high-quality, ML-based error detection models that require minimal human involvement. Our framework consists of two parts: (1) an expressive model to learn rich representations that capture the inherent syntactic and semantic heterogeneity of errors; and (2) a data augmentation model that, given a small seed of clean records, uses dataset-specific transformations to automatically generate additional training data. Our key insight is to learn data augmentation policies from the noisy input dataset in a weakly supervised manner. We show that our framework detects errors with an average precision of ~94% and an average recall of ~93% across a diverse array of datasets that exhibit different types and amounts of errors. We compare our approach to a comprehensive collection of error detection methods, ranging from traditional rule-based methods to ensemble-based and active learning approaches. We show that data augmentation yields an average improvement of 20 F1 points while it requires access to 3 fewer labeled examples compared to other ML approaches. [ACM SIGMOD 2019 doi:10.1145/3299869.3319888].","Heidari, A.; McGrath, J.; Ilyas, I.F.; Rekatsinas, T.",PO,YES,,(1) Constraint Violations (2) HoloClean (3) Outlier Detection (4) Forbidden Item Sets (5) Simple logistic regression,(1) Hospital (2) Food (3) Soccer (4) Adult (5) Animal. The last 3 datasets had the ground truth.,"not mentioned, seems to be any error on string values",-,2 layer NN to detect errors,"Performance, Recall and F1 score of error detection","Slower than compared approaches, but same order of magnitude",,YES,PNUGTE37,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,-,
Interactive correction of mislabeled training data,Academia,IEEE Conference on Visual Analytics Science and Technology (VAST),journalArticle,2019,DC4ML,img,mislabel correction,‚Ä¶ of a label cleaning network. The network is then integrated with a multitask neural network for ‚Ä¶ These methods integrate label cleaning with classification frameworks using deep neural ‚Ä¶,"Xiang, S; Ye, X; Xia, J; Wu, J; Chen, Y; ...",PO,YES,,-,(1) MNIST (2) Clothing,-,-,-,managed to improve against doing nothing,-,% of mislabels,,ZXCPRZGJ,-,10,2.0,2.0,1.0,1.0,0.0,2.0,2.0,2,2.0,0.0,,
Learning functional dependencies with sparse regression,Academia,arxiv,journalArticle,2019,ML4DC,tabular,error detection/repair,‚Ä¶ on the task of weakly supervised data repairing. Recent work [‚Ä¶ -of-the-art machine learning-based data repairing systems. We ‚Ä¶ to automating weakly supervised data preparation ‚Ä¶,"Guo, Z; Rekatsinas, T",Amin,YES,,"PYRO, Reliable Fraction of Information (RFI), Graphical Lasso (GL), ","synthetic dataset, and three noisy dataset: Hospital, Food, and Physician dataset form http://medicare.gov/",-,working only for discrete random variables,"PGM, learning the structure of linear causal networks (undirected) via inverse covariance estimation",better than other approches ,"worse than GL, but equal or better than others","Precision, Recall, and F1",YES,6WFKWVSM,,11,2.0,2.0,1.0,2.0,0.0,2.0,2.0,1,0.0,1.0,-,
Multi-index dialogue data cleaning model,Academia,Joint International Information Technology and Artificial Intelligence Conference (ITAIC),conferencePaper,2019,ML4DC,text,mislabel correction,"‚Ä¶ index dialogue data cleaning model in this paper is based on the unsupervised data cleaning ‚Ä¶ The model is under supervised training. After the training, the model is saved. The score of ‚Ä¶","Ke, X; Bai, J; Wen, L; Cao, B",,YES,https://doi.org/10.1109/ITAIC.2019.8785558,Dual encoder using LSTM,Xiaohuangji dialogue corpus and the Shooter network film script,poorly paired question answer ,,Bi-GRU with attention,"The proposed model has slightly better accuracy (before cleaning:68.8, after: 96.8) than compared model (before: 67.5, after: 96.2)
Also distinguishes the quality of the question-and-answer pair better. as the distribution of predictions probabilities is higher for probabilities that is more close to 0 or 1.",-,"Accuracy
Model prediction results distribution",,A9AM4ZE4,"They have no real contribution in cleaning. Actually, the proposed model (Bi-GRU with attention) that is used in the adopted cleaning framework is performing slightly better than compared model even before data is cleaned.",8,1.0,2.0,1.0,1.0,0.0,2.0,1.0,1,1.0,0.0,,
Learn2Clean: Optimizing the sequence of tasks for web data preparation,Academia,ACM WWW,conferencePaper,2019,"DC4ML, ML4DC",any,holistic,"Data cleaning and preparation has been a long-standing challenge in data science to avoid incorrect results and misleading conclusions obtained from dirty data. For a given dataset and a given machine learning-based task, a plethora of data preprocessing techniques and alternative data curation strategies may lead to dramatically different outputs with unequal quality performance. Most current work on data cleaning and automated machine learning, however, focus on developing either cleaning algorithms or user-guided systems or argue to rely on a principled method to select the sequence of data preprocessing steps that can lead to the optimal quality performance of. In this paper, we propose Learn2Clean, a method based on Q-Learning, a model-free reinforcement learning technique that selects, for a given dataset, a ML model, and a quality performance metric, the optimal sequence of tasks for preprocessing the data such that the quality of the ML model result is maximized. As a preliminary validation of our approach in the context of Web data analytics, we present some promising results on data preparation for clustering, regression, and classification on real-world data.  2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.","Berti-Equille, Laure",PO,YES,10.1145/3308558.3313602,(1) Random data pre-processing selection (2) manual cleaning by a DS expert (3) a auto-ML approach obtained by MLBox (4) no pre-processing (baseline),(1) Google Play Store Apps (2) Google Play Store users (3) House prices,Any error that can be fixed depending on the data preparation tool used.,(1) They compared their approach to manual data cleaning under time constraints (under a minute). I highly doubt the data scientist expert had enough time to clean a significant amount of records. (2) The state seems to be the last data cleaning tool used (3) The performance was on a model with default parameters,"Tabular Q-learning, model-free","Half of the time, they had better or equal results compared to other approaches (e.g. manual cleaning by an expert under time constraints and an auto ML tool)","For data sets with thousands of rows, under a minute.",(ML) Performance of a ML model,YES,,,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,,
AlphaClean: Automatic Generation of Data Cleaning Pipelines,Academia,arxiv,journalArticle,2019,"DC4ML, DC4ML - intro only, ML4DC",any,holistic,"The analyst effort in data cleaning is gradually shifting away from the design of hand-written scripts to building and tuning complex pipelines of automated data cleaning libraries. Hyperparameter tuning for data cleaning is very different than hyperparmeter tuning for machine learning since the pipeline components and objective functions have structure that tuning algorithms can exploit. This paper proposes a framework, called AlphaClean, that rethinks parameter tuning for data cleaning pipelines. AlphaClean provides users with a rich library to define data quality measures with weighted sums of SQL aggregate queries. AlphaClean applies generate-then-search framework where each pipelined cleaning operator contributes candidate transformations to a shared pool. Asynchronously, in separate threads, a search algorithm sequences them into cleaning pipelines that maximize the user-defined quality measures. This architecture allows AlphaClean to apply a number of optimizations including incremental evaluation of the quality measures and learning dynamic pruning rules to reduce the search space. Our experiments on real and synthetic benchmarks suggest that AlphaClean finds solutions of up-to 9x higher quality than naively applying state-of-the-art parameter tuning methods, is significantly more robust to straggling data cleaning methods and redundancy in the data cleaning library, and can incorporate state-of-the-art cleaning systems such as HoloClean as cleaning operators. Copyright  2019, The Authors. All rights reserved.","Krishnan, Sanjay; Wu, Eugene",PO,YES,,(1) Gridsearch (try different tools‚Äô hyperparameter combinaisons) (2) hyperopt (an optimized gridsearch) (3) greedy (tuning each tool independently of others),(1) Hospital (2) London Air Quality (3) Physician. They seem to have dirty and ground truth versions of the data set!,Any,-,They use logistic regression to predict the best data cleaning tool to use on a sample. ,Better results than other traditional hyper-parameter optimization techniques such as Bayesian optimization. Better results than using data cleaning tools alone.,Converges 3x time faster than the next optim. tool on some data set.,(1) Suboptimality (the quotient between the user-defined quality measure for the ground truth data set and the cleaned one) (2) % Errors (the % of errors left during the cleaning process),YES,,,11,2.0,2.0,1.0,2.0,0.0,2.0,2.0,3,2.0,1.0,,
Combining Outlier Detection and Reconstruction Error Minimization for Label Noise Reduction,Academia,IEEE International Conference on Big Data and Smart Computing,conferencePaper,2019,"DC4ML, ML4DC",img,mislabel correction,"Label noise is a common phenomenon when labeling a large-scale dataset for supervised learning. Outlier detection is a recently proposed method to handle this issue by treating the outliers of each class as potential data points with label noise and remove them before training. However, this approach could lead to high false positive rate and hurt the performance. In this paper, we propose a novel and effective method to deal with this issue by combining the strength of outlier detection and reconstruction error minimization (REM). The main idea is add a second verification step (i.e., REM) to the outputs of outlier detection so as to reduce the risk of discarding those points which do not fit the underlying data distribution well but with correct label. Particularly, we first find the outliers in each class by a robust deep autoencoders-based outlier detector, through which not only did we get candidate mislabeled data but also a group of well-learned deep autoencoders. Then a reconstruction error minimization based approach is applied to these outliers to further filter and relabel the mislabeled data. The experimental results on MNIST dataset show that the proposed method could significantly reduce the false positive rate of outlier detection and improve the performance of both data cleaning and classification in the presence of label noise.","Zhang, WN; Tan, XY; IEEE",PO,YES,,"For goal (1): other approaches that can re-label a dataset (1.1) ICCN-SMO (1.2) TC-SVM (1.3) ALNR (1.4) LN-RDA, for goal (2): robust cleaning approaches and some of (1) solutions",MNIST,mislabels,-,robust auto-encoder,Superior to other methods for almost every level of label noise,Significantly lower than other methods,(1) Accuracy of fixing mislabels (2) perfomance of the end classifier,,,short and sweet,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,-,
Confident learning: Estimating uncertainty in dataset labels,Both,Journal of Artificial Intelligence Research,journalArticle,2019,"DC4ML, ML4DC",any,mislabel correction,"Learning exists in the context of data, yet notions of confidence typically focus on model predictions, not label quality. Confident learning (CL) is an alternative approach which focuses instead on label quality by characterizing and identifying label errors in datasets, based on the principles of pruning noisy data, counting with probabilistic thresholds to estimate noise, and ranking examples to train with confidence. Whereas numerous studies have developed these principles independently, here, we combine them, building on the assumption of a class-conditional noise process to directly estimate the joint distribution between noisy (given) labels and uncorrupted (unknown) labels. This results in a generalized CL which is provably consistent and experimentally performant. We present sufficient conditions where CL exactly finds label errors, and show CL performance exceeding seven recent competitive approaches for learning with noisy labels on the CIFAR dataset. Uniquely, the CL framework is not coupled to a specific data modality or model (e.g., we use CL to find several label errors in the presumed error-free MNIST dataset and improve sentiment classification on text data in Amazon Reviews). We also employ CL on ImageNet to quantify ontological class overlap (e.g., estimating 645 missile images are mislabeled as their parent class projectile), and moderately increase model accuracy (e.g., for ResNet) by cleaning data prior to training. These results are replicable using the open-source cleanlab release. Copyright  2019, The Authors. All rights reserved.","Northcutt, Curtis G.; Jiang, Lu; Chuang, Isaac L.",PO,YES,,"Different approaches to improve model performances in presence of label noise (one data cleaning approach, other robust ML approaches) and a baseline (i.e. a vanilla model, without data cleaning or robust ML). They also compare against the random removal of data.",(1) CIFAR (2) MNIST (3) Amazon reviews (4) ImageNet,mislabels,-,depends on the dataset,open this page for details,-,"(1) test accuracy of ML model trained on the cleaned dataset (2) Accuracy, F1, precision, recall of errors found in a data set",,,,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,,
A hybrid data cleaning framework using Markov logic networks,Academia,IEEE Transactions on Knowledge and Data Engineering,journalArticle,2019,ML4DC,tabular,error detection/repair,"With the increase of dirty data, data cleaning turns into a crux of data analysis. Most of the existing algorithms rely on either qualitative techniques (e.g., data rules) or quantitative ones (e.g., statistical methods). In this paper, we present a novel hybrid data cleaning framework on top of Markov logic networks (MLNs), termed as MLNClean, which is capable of cleaning both schema-level and instance-level errors. MLNClean mainly consists of two cleaning stages, namely, first cleaning multiple data versions separately (each of which corresponds to one data rule), and then deriving the final clean data based on multiple data versions. Moreover, we propose a series of techniques/concepts, e.g., the MLN index, the concepts of reliability score and fusion score, to facilitate the cleaning process. Extensive experimental results on both real and synthetic datasets demonstrate the superiority of MLNClean to the state-of-the-art approach in terms of both accuracy and efficiency. Copyright  2019, The Authors. All rights reserved.","Gao, Yunjun; Ge, Congcong; Miao, Xiaoye; Wang, Haobo; Yao, Bin; Li, Qing",Dima,YES,,"HoloClean (T. Rekatsinas, X. Chu, I. F. Ilyas, and C. R ÃÅe, ‚ÄúHoloclean: Holistic datarepairs with probabilistic inference,‚ÄùPVLDB, vol. 10, no. 11, pp. 1190‚Äì1201, 201)","https://data.medicare.gov/data/hospital-comparis a real dataset that provides information about healthcare associated infections occurred in hospitals. It contains 231,265 tuples.                                        https://www.cars.com/ contains the used vehicle information, including model,make, type, year, condition, wheel Drive, doors. It consists of 30,760 tuples.                                                                         https://www.notion.so/ce4c3e5544e34df5ac2be15c79938c76?pvs=21 is a benchmark for performance metrics for systems operating.",,"For MLNClean, there are two reasons for the decline. The first reason is that, with the increase of error percentage, AGP is prone to wrongly treat more normal groups as abnormal ones, No replication package",Evaluation only in terms of the repaired attributes,"F1 score (where precision is equal to the ratio of correctly repaired attribute values to the total number of updated attribute values, and recall equals the ratio of correctly repaired attribute values to the total number of erroneous values)",Runtime,"reliability score, ",,,Extensive experimental results on both real and synthetic datasets demonstrate the superiority of MLNClean to the state-of-the-art approach in terms of both accuracy and efficiency. Very thorough evaluation indeed!,12,2.0,2.0,2.0,2.0,1.0,1.0,2.0,3,2.0,1.0,,
Data cleansing for models trained with SGD,Academia,NeurIPS,journalArticle,2019,"DC4ML, ML4DC",img,mislabel correction,"Data cleansing is a typical approach used to improve the accuracy of machine learning models, which, however, requires extensive domain knowledge to identify the influential instances that affect the models. In this paper, we propose an algorithm that can suggest influential instances without using any domain knowledge. With the proposed method, users only need to inspect the instances suggested by the algorithm, implying that users do not need extensive knowledge for this procedure, which enables even non-experts to conduct data cleansing and improve the model. The existing methods require the loss function to be convex and an optimal model to be obtained, which is not always the case in modern machine learning. To overcome these limitations, we propose a novel approach specifically designed for the models trained with stochastic gradient descent (SGD). The proposed method infers the influential instances by retracing the steps of the SGD while incorporating intermediate models computed in each step. Through experiments, we demonstrate that the proposed method can accurately infer the influential instances. Moreover, we used MNIST and CIFAR10 to show that the models can be effectively improved by removing the influential instances suggested by the proposed method. Copyright  2019, The Authors. All rights reserved.","Hara, Satoshi; Nitanday, Atsushi; Maehara, Takanori",PO,YES,,(1) Influence function of K&L (2) two outlier detection techniques (auto-encoder and isolation forest),(1) MNIST (2) CIFAR,any,-,(1) logistic regression and (2) 2-layer NN,The only method (in the compared approaches) that achieve statistically significant improvements,-,misclassification rate (i.e. 1-accuracy) of the end model,,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,2.0,0.0,https://github.com/sato9hara/sgd-influence,
Raha: A Configuration-Free Error Detection System,Academia,ACM SIGMOD,conferencePaper,2019,ML4DC,tabular,error detection,,"Mahdavi, Mohammad; Abedjan, Ziawasch; Fernandez, Raul Castro; Madden, Samuel; Ouzzani, Mourad; Stonebraker, Michael; Tang, Nan",PO,YES,,,"Most of them have both the GT and dirty versions: Hostpital, Flights, Address, Beers, Rayyan, Movies, IT, Tax","Any (it depends on the primary error detection tools used); missing values, formatting issues, etc.",(1) The quality of the predictions and the time it takes to complete depends on the primary error detection tool used,"They tried multiple error-detection models: AdaBoost, Decision Trees, Gradient Boosting, Gaussian Naive Bayes, SVM",between .8 to 1,between 100 secs to 1000 secs for datasets of size 20k lines and (?) lines respectively,F1 score of detected errors (this is a error detection tool only),YES,,,14,2.0,2.0,2.0,2.0,2.0,2.0,2.0,4,2.0,2.0,,
Deep Learning based Radial Blur Estimation and Image Enhancement,Academia,IEEE International Conference on Electronics Computing and Communication Technologies (CONECCT),conferencePaper,2019,"DC4ML, DC4ML - intro only, ML4DC",img,error detection/repair,"In this paper, we propose a deep learning based pipeline to estimate the radial blur and enhance the deblurred image. The radial blur is introduced in the image as an effect of ego motion in autonomous vehicle systems. The deblurring of the image with radial blur is challenging since most of the blur models do not estimate radial blur. Hence, we design a deep learning based pipeline with estimation and enhancement modules. The estimation module is designed with CuratorNet to estimate radial PSF in two stages. The estimated PSF is used for deblurring of input radial blurred images. The enhancement module is designed with convolutional autoencoder which enhances the deblurred image to remove artefacts in order to detect the traffic signs. We demonstrate the results of the proposed pipeline on synthetic and real images with traffic signs and compare the results with existing methods.  2019 IEEE.","Hurakadli, Vaishnavi; Kulkarni, Sujaykumar; Patil, Ujwala; Tabib, Ramesh; Mudengudi, Uma",Dima,YES,10.1109/CONECCT47791.2019.9012864,,Custom dataset,,,,,,PSNR and SSIM authors do not say what it is!!,,,,11,2.0,2.0,2.0,1.0,1.0,2.0,1.0,2,1.0,1.0,,
AutoBlock: A hands-off blocking framework for entity matching,Both,International Conference on Web Search and Data Mining,journalArticle,2019,ML4DC,tabular,entity matching / duplicate removal,"Entity matching seeks to identify data records over one or multiple data sources that refer to the same real-world entity. Virtually every entity matching task on large datasets requires blocking, a step that reduces the number of record pairs to be matched. However, most of the traditional blocking methods are learning-free and key-based, and their successes are largely built on laborious human effort in cleaning data and designing blocking keys. In this paper, we propose AutoBlock, a novel hands-off blocking framework for entity matching, based on similarity-preserving representation learning and nearest neighbor search. Our contributions include: (a) Automation: AutoBlock frees users from laborious data cleaning and blocking key tuning. (b) Scalability: AutoBlock has a sub-quadratic total time complexity and can be easily deployed for millions of records. (c) Effectiveness: AutoBlock outperforms a wide range of competitive baselines on multiple large-scale, real-world datasets, especially when datasets are dirty and/or unstructured. Copyright  2019, The Authors. All rights reserved.","Zhang, Wei; Dong, Xin Luna; Wei, Hao; Faloutsos, Christos; Sisman, Bunyamin; Page, David",PO,YES,,(1) Key-based blocking (2) MinHash blocking (3) DeepER,(1) Movie (from Amazon and Wikipedia) (2) Music (from IMDb and WikiData) (3) Grocery (from Amazon and ShopFoodEx),duplicates,-,NN and attention mechanisms,slightly better / comaparable,no comparison with other approaches,(1) recall (2) p/e ratio,,,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,,
Detecting annotation noise in automatically labelled data,Academia,Annual Meeting of the Association for Computational Linguistics,conferencePaper,2018,"DC4ML, ML4DC",text,mislabel correction,"‚Ä¶ We provide a novel approach to error detection that is able to identify errors in automatically ‚Ä¶ We show how AL can be used to guide an unsupervised generative model, and we will ‚Ä¶","Rehbein, I; Ruppenhofer, J",,YES,http://dx.doi.org/10.18653/v1/P17-1107,"Majority vote
Multi-Annotator Competence Estimation
(MACE)",standard newspaper text (English Penn Treebank and English Web treebank),Annotation error (in automatically annotated text),"Most of failed cases are where an adjective (JJ) was mistaken for a past participle (VBN). Also, since it depends on human, the model is affected by under-specified/unclear cases for humans. ",Adopted the generative model (MACE) in Active Learning  ,"Testing on POS: 
Error detection accuracy is significantly improved using proposed approach (after 1000 iterations: 95.1 to 98.6) compared to majority vote and MACE where both had almost similar accuracy (87.4 to 93.9)
using variational inference (VI - AL) gives a substantially higher precision and recall when guiding AL compared to using QBC (difference in precision between 6% to 10% and 10% to 18% in recall)",-,"Accuracy, Recall and Precision",,ASQGRQHN,,9,2.0,2.0,1.0,1.0,1.0,1.0,1.0,4,2.0,2.0,http://www.cl/.http://uni-heidelberg.de/%CB%9Crehbein/resources (not found),
Automated Cleaning of Identity Label Noise in A Large-scale Face Dataset Using A Face Image Quality Control,Academia,proquest,thesis,2018,ML4DC,img,mislabel correction,"‚Ä¶ We also apply our identity label cleaning method on a subset of ‚Ä¶ Due to recent advances in using the deep learning techniques ‚Ä¶ In the current automated id label cleaning methods, low-‚Ä¶","Guo, G; Adjeroh, D; Li, X",PO,YES,,semantic bootstrapping (see [5]) in paper,MS-Celeb-1M.v1 dataset,mislabels,-,SVM and CNNs,-,-,-,,PYXVZK5E,Poor English. Requires manually selecting threshold; thus it is not scalable.,8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,1.0,1.0,-,
DeepClean: Data Cleaning via Question Asking,Academia,International Conference on Data Science and Advanced Analytics,journalArticle,2018,DC4ML,tabular,error detection/repair,"‚Ä¶ in ¬ßIV, to generate questions effective for data cleaning, it is ‚Ä¶ attribute a‚àó , we apply a weakly supervised reward- guided ‚Ä¶ The input to this neural network is the sequence concatenated ‚Ä¶","Zhang, X; Ji, Y; Nguyen, C; Wang, T",PO,YES,,Katara,"(1) WikiTables, (2) DBPedia, (3) WebTables, and (4) RelationalTables",any,-,(1) BiLSTM to predict ‚Äúcorrelation‚Äù between attributes (open page) (2) DrQA for QA engine,Comparable to Katara,-,Precision and recall of error detection ,,LCE89FRZ,I considered it to be ML 4 DC because DrQA is a central component (and it uses ML (NLP) to query Wikipedia),12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,1,0.0,1.0,-,
Performance analysis of machine learning algorithms for missing value imputation,Academia,International Journal of Advanced Computer Science and Applications,journalArticle,2018,ML4DC,tabular,imputation,"Data mining requires a pre-processing task in which the data are prepared, cleaned, integrated, transformed, reduced and discretized for ensuring the quality. Missing values is a universal problem in many research domains that is commonly encountered in the data cleaning process. Missing values usually occur when a value of stored data absent for a variable of an observation. Missing values problem imposes undesirable effect on analysis results, especially when it leads to biased parameter estimates. Data imputation is a common way to deal with missing values where the missing value's substitutes are discovered through statistical or machine learning techniques. Nevertheless, examining the strengths (and limitations) of these techniques is important to aid understanding its characteristics. In this paper, the performance of three machine learning classifiers (K-Nearest Neighbors (KNN), Decision Tree, and Bayesian Networks) are compared in terms of data imputation accuracy. The results shows that among the three classifiers, Bayesian has the most promising performance.","Abidin, N.Z.; Ismail, A.R.",PO,YES,,(1) KNN (2) Bayesian Networks (3) Decision Trees,Open up to see the full list,missing values,-,(1) KNN (2) Bayesian Networks (3) Decision Trees,"Bayesian Network is usually better, but prohibitively expensive on large datasets. Second best is DT.",BN is expensive on large datasets,Different metrics to evaluate the error of the imputation method for missing values for numerical fields: (1) MAE (2) MSE (3) RMSE ,,AMPP66IC,-,0,,,,,,,,0,,,-,
Cleaning Crowdsourced Labels Using Oracles For Supervised Learning,Academia,VLDB Endowment,conferencePaper,2018,ML4DC,tabular,mislabel correction,"‚Ä¶ supervised learning algorithm for noisy labels. Instead, we aim to develop a label cleaning ‚Ä¶ the best use of oracle-based cleaning for supervised learning. As shown in Figure 1, suppose ‚Ä¶","Dolatshah, M; Teoh, M; Wang, J; Pei, J",PO,YES,,"(1) Random selection of instances (2) Clean instances with highest label noise (not considering its impact on the end model) (3) ActiveClean (4) Uncertainty sampling (i.e. clean the instance the model is the most uncertain about) (5) Expected Error Reduction (similar to what they are doing, read paper for more details) (6) Hung","Pure synthetic datasets (i.e. features and labels): (1.1) 2-d gaussian (small) (1.2) 2-d gaussian (large). Real datasets with synthetic label noise: (2.1) Heart (2.2) diabetes (2.3) cancer. Real dataset, real crowdsource labeling: (3.1) restaurant",mislabel,-,-,It generally is better (i.e. it improve model‚Äôs performances the fastest and the most),"Very poor, to return only on instance to be relabeled, models without one instance in the dataset must be trained (i.e. if dataset size is 10, then 10 models must be trained to find the best dataset version). They provide an optimization which they do not explain (pruning)",Accuracy of the end model,,GPE4HXJT,"The paper is way too complicated for what they did. Furthermore, the contribution is almost trivial and the method is highly inefficient. At each iteration, x model must be trained, where x is the size of the dataset.",8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,2.0,0.0,,
A comparative evaluation of outlier detection algorithms: Experiments and analyses  ,Both,Pattern Recognit.,journalArticle,2018,ML4DC,tabular,outliers detection,"We survey unsupervised machine learning algorithms in the context of outlier detection. This task challenges state-of-the-art methods from a variety of research fields to applications including fraud detection, intrusion detection, medical diagnoses and data cleaning. The selected methods are benchmarked on publicly available datasets and novel industrial datasets. Each method is then submitted to extensive scalability, memory consumption and robustness tests in order to build a full overview of the algorithms characteristics.  2017 Elsevier Ltd","Domingues, Remi; Filippone, Maurizio; Michiardi, Pietro; Zouaoui, Jihane",,YES,10.1016/j.patcog.2017.09.037,"14 methods categorized into:
* probabilistic methods (most of them falls here)
* distance based
* Neighbor based
* information theory
* Neural Networks
* Domain based 
* Isolation methods","15 datasets (synthetic and real) ranging from 723 to 20,000 samples and containing from 6 to 107 features (12 public and 3 proprietary)",outliers,,NA (comparative),"basic results:

robustness: robustness measures on synthetic datasets confirm the poor performance of abod and gwr. 
Good average results were observed for iforest, ocsvm, lof, rkde, dpgmm and gmm. The nearest-neighbor-based methods showed difficulties in handling datasets with a high background noise.","memory:  ocsvm, gwr and abod have the best memory requirements and
scalability and never exceed 250MB RAM, at the cost of a higher
computation time

computation time: iforest and lsa show a very good training and prediction time scaling for both increasing number of features and samples, along with a very small base computation time. dpgmm, gmm and bgm scale well on datasets with a large number of samples and thus could be suitable for systems where fast predictions matter. The base computation time of dpgmm is however an important issue when the number of features becomes higher than a hundred. rkde, ocsvm and sod which have good outlier detection performance on real
datasets are thus computationally expensive, which adds interest to iforest, dpgmm and simpler models such as gmm, kl, ppca or Mahalanobis.","ROC AUC and PR AUC 
robustness (for increasing dimensionality, samples, and noise density), scalability (computation and prediction time when increasing dimensionality and samples), memory usage (for increasing dimensionality, samples)",,PZLJ5JTE,,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,2,0.0,2.0,,
Finding and correcting syntax errors using recurrent neural networks,Academia,PeerJ,journalArticle,2017,ML4DC,text,error detection/repair,"‚Ä¶ To approximate such a function, we used machine learning to map contexts to categorical ‚Ä¶ We describe a two-LSTM system, and its syntax error detection strategy. We then describe our ‚Ä¶","Santos, EA; Campbell, JC; Hindle, A; Amaral, JN",,YES,https://doi.org/10.7287/peerj.preprints.3123v1,n-gram model of order 10,Built corpus from over 9000 JavaScript repositories mined from Github,Syntax Error,"Low speed for providing suggestions (6 seconds)
Scalability Issue
Evolvement of language definitions cause the tool to fail to find the appropriate syntax error where it otherwise was able to produce a valid fix
",Two LSTM models: one forward and one backward,"correct location of the syntax error: 54.74% in its top 4 suggestions 
exact fix up to 35.50% of the time",- (although they mentioned it is slow they did not show or discuss experiments related to this),"Mean Reciprocal Rank (MRR) for finding syntax error
Percentage and raw number of valid fixes
",,3S8ZRTJP,"not peer reviewed 
shows only the final results when comparing to the other approach but does not show any details or conducted experiments ",9,1.0,1.0,1.0,1.0,2.0,2.0,1.0,2,0.0,2.0,https://github.com/eddieantonio/training-grammar-guru/blob/icsme2017/bin/detect.py,
Holoclean: Holistic data repairs with probabilistic inference,Academia,arxiv,journalArticle,2017,ML4DC,tabular,error repair,"‚Ä¶ First, we focus on data repairing methods that rely on integrity constraints [5, 8, 12]. These ‚Ä¶ ‚Ä¢ SCARE [39]: This is a state-of-the-art data cleaning method that relies on machine learning ‚Ä¶","Rekatsinas, T; Chu, X; Ilyas, IF; R√©, C",PO,YES,,(1) Holistic (2) Katara (3) SCARE,(1) Hospital (2) Flights (3) Food (4) Physicians. All with ground truth values (either it was already available or they manually labeled it),wrong values,-,Factor graph,Holoclean consistently outperforms other approaches,Slower than compared approaches (mostly because it combines other approaches),(1) Precision (i.e. the fraction of correct repairs) (2) Recall (3) F1 Score,YES,PGFQMSFN,-,12,2.0,2.0,2.0,2.0,0.0,2.0,2.0,4,2.0,2.0,https://github.com/HoloClean/holoclean,
Label denoising based on Bayesian aggregation,Academia,International Journal of Machine Learning and Cybernetics,journalArticle,2017,ML4DC,tabular,mislabel correction,"Label noise is a common problem that affects supervised learning and can produce misleading results. It is shown that only 5% of switched labels lead to a decrease of performances. Therefore, the true class of an instance must be distinguished from its observed label. In the past decade, classification in presence of label noise was the topic of interest. Several scholars focused on kNN-based approaches for data cleansing. These types of approaches often are susceptible to high label noise rate and when a batch of instances with noisy labels are exist they may deteriorate the results. The problem arises since the methods have a local view of instances. Another approach is to have a global view of instances. In a global view, instances with large distance from their respective classes are detected as noisy. A potential problem however is the determination of a threshold. An inappropriate threshold may lead to detection of a correct instance as noisy instance. In this paper a new method for label denoising based on Bayesian aggregation is proposed which solves the problems of kNN-based approaches by aggregating the local and global views of instances. The aggregation of local and global information leads to a more robust and accurate detection of instances with noisy labels and estimation of their true labels. The experimental results show the capabilities and robustness of the proposed method.  2015, Springer-Verlag Berlin Heidelberg.","Bagherzadeh, Parsa; Sadoghi Yazdi, Hadi",Dima,YES,10.1007/s13042-015-0474-y,"IB2, DROP, CCTree, MCTree, WkNN, KDBMS","UCI data sets (https://archive.ics.uci.edu/ml/index.php): BUPA, Ionosphere, Alabone, Pen digits",,Compared approaches are dated from 90ies,SVM with linear kernel,"Precision, Sensitivity, F1",no such performance metrics,Ranking algorithms based on the Friedman test ,,IRWZDJP4,,9,1.0,2.0,1.0,2.0,0.0,1.0,2.0,4,2.0,2.0,,
Data Improving in Time Series Using ARX and ANN Models,Academia,IEEE Transactions on Power Systems,journalArticle,2017,"DC4ML, ML4DC",tabular,error detection/repair,"Anomalous data can negatively impact energy forecasting by causing model parameters to be incorrectly estimated. This paper presents two approaches for the detection and imputation of anomalies in time series data. Autoregressive with exogenous inputs (ARX) and artificial neural network (ANN) models are used to extract the characteristics of time series. Anomalies are detected by performing hypothesis testing on the extrema of the residuals, and the anomalous data points are imputed using the ARX and ANN models. Because the anomalies affect the model coefficients, the data cleaning process is performed iteratively. The models are re-learned on ‚Äúcleaner‚Äù data after an anomaly is imputed. The anomalous data are reimputed to each iteration using the updated ARX and ANN models. The ARX and ANN data cleaning models are evaluated on natural gas time series data. This paper demonstrates that the proposed approaches are able to identify and impute anomalous data points. Forecasting models learned on the unclean data and the cleaned data are tested on an uncleaned out-of-sample dataset. The forecasting model learned on the cleaned data outperforms the model learned on the unclean data with 1.67% improvement in the mean absolute percentage errors and a 32.8% improvement in the root mean squared error. Existing challenges include correctly identifying specific types of anomalies such as negative flows.",H. N. Akouemo; R. J. Povinelli,Dima,YES,10.1109/TPWRS.2017.2656939,,,,,,,,,,,Good for snowbowling,10,1.0,2.0,2.0,1.0,1.0,1.0,2.0,2,1.0,1.0,,
Automated data cleansing through meta-learning,Both,AAAI,conferencePaper,2017,"DC4ML, DC4ML - intro only, ML4DC",any,holistic,"Data preprocessing or cleansing is one of the biggest hurdles
in industry for developing successful machine learning applications. The process of data cleansing includes data imputation, feature normalization & selection, dimensionality reduction, and data balancing applications. Currently such preprocessing is manual. One approach for automating this process
is meta-learning. In this paper we experiment with state of
the art meta-learning methodologies and identify the inadequacies and research challenges for solving such a problem.","Gemp, Ian; Theocharous, Georgios; Ghavamzadeh, Mohammad",PO,YES,,None,(1) Scikit-Learn‚Äôs make classification method (2) Adobe‚Äôs internal dataset (3) classification tasks downloaded from http://openml.org/,-,Does not works. I suppose the meta feature they use are bad. They should have used embeddings or a model that directly output similarity scores.,-,-,-,"Normalized relative rank (worst results of the experiments tend toward 0, while the best, 1)",YES,,,9,2.0,2.0,1.0,2.0,0.0,1.0,1.0,4,2.0,2.0,,
Numerically grounded language models for semantic error correction,Academia,arxiv,conferencePaper,2016,ML4DC,text,error detection/repair,‚Ä¶ Semantic error detection and correction is an important task for applications such as fact ‚Ä¶ Our contributions are: 1) a straightforward extension to recurrent neural network (RNN) ‚Ä¶,"Spithourakis, GP; Augenstein, I; Riedel, S",,YES,http://dx.doi.org/10.18653/v1/D16-1101,"1. random scorer: assigns random scores from a uniform distribution. 
2 always (never) scorers: assign the lowest (highest) score to the original document and uniformly random scores to the corrections.
3. base-LM (a single-layer LSTM)
4. LM grounded in numeric quantities mentioned inline with text 
5. LM conditioned on a potentially incomplete KB 
6. 4&5",clinical records from the London Chest Hospital,Semantical ,,"LM grounded in numeric quantities mentioned inline with text 
LM conditioned on a potentially incomplete KB 
6. grounded-conditional (g-conditional)"," g-conditional models has the best results and improve the base-LM. While, grounded models has better results than base-LM, conditional models has worse results. 
Compared to baseline (random, always, never), LM-based models improved the results significantly.",-,"Mean Average Precision (MAP)
Precision
Recall
F1
perplexity (PP) 
adjusted perplexity (APP)",,5VI3A7PB,"they compared to baseline (random, always, never) as no previous approach has explored inline grounded numbers",9,2.0,2.0,1.0,2.0,0.0,1.0,1.0,2,0.0,2.0,,
Improving Data Quality by Leveraging Statistical Relational Learning.,Academia,ICIQ,journalArticle,2016,DC4ML,tabular,error detection/repair,"‚Ä¶ SRL is a branch of machine learning that models joint distributions over relational data. ‚Ä¶ We define data cleaning rules in the form of CFDs and MDs. For example, we express œÜ as ‚Ä¶","Visengeriyeva, L; Akbik, A; Kaul, M; Rabl, T; Markl, V",PO,YES,,against using integrity constraints individually,(1) hospital (2) TPC-h (3) MSAG,anything that can be discrovered by integrity constraints,-,probabilistic graphical model,better than compared approaches (obvs),-,precision recall f1,YES,KPW96E84,"not worth covering in details, just mention it in the SLR.",8,2.0,2.0,1.0,1.0,0.0,1.0,1.0,2,0.0,2.0,-,
Asking for a second opinion: Re-querying of noisy multi-class labels,Industry,ICASSP,journalArticle,2016,DC4ML,any,mislabel correction,‚Ä¶ several different scenarios including data cleansing as a pre‚Ä¶ represents most large-scale supervised learning tasks. We ‚Ä¶ nonlinear classifier such as a neural network or decision tree. In ‚Ä¶,"Stokes, JW; Kapoor, A; Ray, D",Dima,YES,,,,,,,,,maximum negative margin,,DH9SVUGN,,10,1.0,2.0,1.0,2.0,1.0,1.0,2.0,2,1.0,1.0,,
WENN for individualized cleaning in imbalanced data,Academia,International Conference on Pattern Recognition (ICPR),conferencePaper,2016,"DC4ML, DC4ML - intro only",any,outliers detection,"This paper proposes individualized cleaning for diverse imbalanced data sets. Existing techniques for data cleaning have difficulties with rare cases and outliers in minority class, especially, in highly unbalanced data. The drawback leads incomplete and imprecise examples to removal. In order to enhance the robustness and perform thorough data cleaning, we propose a weighted edited nearest neighbor (WENN), which detects and removes noisy examples from both classes intelligently. It considers individual characteristics of each imbalanced data, involving global class imbalance and local distribution. The main idea of the proposed method is to carefully put more focus on the majority class than the minority class during data cleaning. Extensive experiments over synthetic and real data clearly validate the superiority of our approach against other data cleaning methods.","Hongjiao Guan; Yingtao Zhang; Min Xian; Cheng, H.D.; Xianglong Tang",PO,YES,10.1109/ICPR.2016.7899676,(1) ENN (the method they improve) (2) IPF ,a lot of real and synthetic ones,any (outliers),-,-,"on average, slightly better on real datasets. On synthetic ones, it is clearly better..",-,AUC (tp rate vs fp rate for different classification thresholds) and sensitivity (whatever that is - not explained),,,,7,1.0,1.0,1.0,2.0,0.0,1.0,1.0,2,2.0,0.0,,
Entity Resolution Using Convolutional Neural Network,Academia,Procedia Comput. Sci.,journalArticle,2016,ML4DC,img,entity matching / duplicate removal,"Entity resolution is an important application in field of data cleaning. Standard approaches like deterministic methods and probabilistic methods are generally used for this purpose. Many new approaches using single layer perceptron, crowdsourcing etc. are developed to improve the efficiency and also to reduce the time of entity resolution. The approaches used for this purpose also depend on the type of dataset, labeled or unlabeled. This paper presents a new method for labeled data which uses single layered convolutional neural network to perform entity resolution. It also describes how crowdsourcing can be used with the output of the convolutional neural network to further improve the accuracy of the approach while minimizing the cost of crowdsourcing. The paper also discusses the data pre-processing steps used for training the convolutional neural network. Finally it describes the airplane sensor dataset which is used for demonstration of this approach and then shows the experimental results achieved using convolutional neural network.  2016 The Authors.","Gottapu, Ram Deepak; Dagli, Cihan; Ali, Bharami",PO,YES,10.1016/j.procs.2016.09.306,Jaccard similarity (more on that on the Notion page),A private sensor description dataset from Boeing,duplicates,-,CNN,"53% accuracy, better than Jaccard‚Äôs similarity",8 times slower than Jaccard‚Äôs similarity,accuracy,,,‚Ä¶,9,2.0,2.0,2.0,1.0,0.0,1.0,1.0,2,0.0,2.0,-,
ActiveClean: An interactive data cleaning framework for modern machine learning,Academia,ACM SIGMOD,conferencePaper,2016,"DC4ML, ML4DC",tabular,error detection/repair,"Databases can be corrupted with various errors such as missing, incorrect, or inconsistent values. Increasingly, modern data analysis pipelines involve Machine Learning, and the effects of dirty data can be difficult to debug. Dirty data is often sparse, and naive sampling solutions are not suited for high-dimensional models. We propose ActiveClean, a progressive framework for training Machine Learning models with data cleaning. Our framework updates a model iteratively as the analyst cleans small batches of data, and includes numerous optimizations such as importance weighting and dirty data detection. We designed a visual interface to wrap around this framework and demonstrate ActiveClean for a video classification problem and a topic modeling problem.  2016 ACM.","Krishnan, Sanjay; Franklin, Michael J.; Goldberg, Ken; Wang, Jiannan; Wu, Eugene",PO,YES,10.1145/2882903.2899409,"(1) 2 methods that randomly sample instances, (2) Active learning (i.e. selecting instances to clean based on the uncertainty of the model) (3) Oracle (i.e. the fastest way to improve model performances) (4) No sampling (i.e. no cleaning)",(1) Real datasets: (1.1) IMDB (1.2) Dollar for docs (3) (2) Synthetic datasets (2): (2.1) Income ClassiÔ¨Åcation (Adult) and (2.2) Seizure ClassiÔ¨Åcation (EEG) (2.3) MNIST numbers,"It could support any, but in their experiment, they used rule-based detection and outlier detection.",(1) The approach is only compatible with convex-loss models ,The models used to select samples to clean (based on its gradient) are convex-loss models.,Converges faster to a lower level of (1) model error and (2) test error,"generally converges faster than other approaches - time, complexity, etc. not mentioned","For the real datasets (1): (1.1) Model error, which is the distance between the trained model and true
model if all data were cleaned Œ∏ ‚àí Œ∏(c), and (1.2) test error, which is the prediction accuracy of the model on a held-out set of clean data.
For the synthetic datasets (2): (2.1) test error and (2.2) % of detected errors",,,,8,1.0,2.0,1.0,1.0,1.0,1.0,1.0,4,2.0,2.0,,
